Title: Enhancing Deep Learning Interpretability with GMM-MI: A Mutual Information Estimation Approach

Abstract:
In the context of deep learning, understanding the complex interactions within neural networks remains a significant challenge. To address this, we introduce GMM-MI, a robust mutual information estimator grounded in Gaussian Mixture Models (GMMs). By accurately quantifying the mutual information between latent representations and input data, GMM-MI provides a nuanced understanding of representation learning and disentanglement in deep neural networks. Our approach leverages the flexibility of GMMs to model complex distributions, thereby enhancing the interpretability of deep learning models. Through empirical evaluations, we demonstrate the efficacy of GMM-MI in estimating mutual information and its application in representation learning. The results underscore the potential of GMM-MI to illuminate the inner workings of deep neural networks, contributing to the development of more transparent and explainable AI systems. As of 2022, this work advances the field of deep learning interpretability.