**Abstract: Enhancing Interpretability in Deep Learning for Physical Systems via Mutual Information Estimation with Gaussian Mixture Models**

Deep learning models are increasingly employed in physics research for tasks ranging from simulation acceleration to data analysis, yet their inherent “black box” nature limits interpretability and hinders scientific understanding. This work introduces GMM-MI, a novel estimator for mutual information (MI) designed to address this critical limitation. Leveraging Gaussian Mixture Models (GMMs), we provide a robust and computationally efficient method for quantifying the dependence between input and output features within deep neural networks. Traditional MI estimation techniques often struggle with high-dimensional data and complex distributions, leading to inaccuracies and instability. GMM-MI offers a significant improvement by approximating the underlying data distribution and mitigating these challenges. 

We demonstrate the efficacy of GMM-MI across diverse synthetic datasets representative of physical phenomena, showcasing its ability to identify salient input features driving model predictions.  Ultimately, this approach facilitates a deeper understanding of the learned representations and enhances the trustworthiness of deep learning models within the physical sciences, promoting more informed scientific discovery.  Further investigation is warranted to assess its generalizability to real-world datasets and integration with representation learning techniques.