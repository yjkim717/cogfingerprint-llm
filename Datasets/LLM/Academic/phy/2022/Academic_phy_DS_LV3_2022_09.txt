In computational physics, the interpretability of deep learning models remains a critical challenge for extracting physically meaningful insights from complex systems. This work introduces GMM-MI, a robust mutual information estimator grounded in Gaussian mixture models, to quantitatively evaluate latent representations in physics-informed neural networks. By leveraging the statistical rigor of Gaussian mixtures, GMM-MI provides an information-theoretic framework to measure feature disentanglement and nonlinear associations within learned embeddings of high-dimensional physical data, such as turbulent flow fields or quantum many-body states. Our 2022 benchmark studies demonstrate that this approach not only elucidates the underlying physical mechanisms captured by deep networks but also enhances model trustworthiness by identifying spurious correlations and ensuring alignment with known conservation laws. Consequently, GMM-MI bridges representation learning and domain-specific interpretability, offering a principled methodology for advancing data-driven discovery in physics where mechanistic understanding is paramount.