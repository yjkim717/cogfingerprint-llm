Of particular interest in modern physics is the development of interpretable machine learning methods for analyzing complex physical systems, where understanding the relationships between learned latent variables is paramount. This work introduces GMM-MI, a robust estimator for mutual information (MI) grounded in Gaussian Mixture Models, designed to quantify statistical dependencies within the latent representations of deep learning models. By accurately modeling the underlying probability distributions of high-dimensional data, GMM-MI overcomes the limitations of existing MI estimators, which often exhibit high variance or bias. We demonstrate its efficacy in a physics context by applying it to representations learned from particle collision data and condensed matter systems, revealing interpretable correlations between latent variables and physical observables. The estimator provides a principled, information-theoretic framework for validating that a model has learned physically meaningful features, rather than spurious correlations. Our results, benchmarked against established methods, confirm that GMM-MI offers superior reliability for representation analysis, thereby advancing the interpretability of deep learning applications in theoretical and experimental physics.