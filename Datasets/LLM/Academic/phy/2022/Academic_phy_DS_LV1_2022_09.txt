**Abstract**

The interpretability of deep learning models remains a critical challenge in computational physics, particularly for applications involving complex, high-dimensional data. While latent representations often capture physically meaningful structures, quantifying these relationships has proven difficult with existing mutual information (MI) estimators, which struggle with the non-Gaussian distributions common in physical systems. Here, we present a novel methodology leveraging Gaussian Mixture Models (GMMs) to estimate MI between learned latent variables, providing a robust tool for representation analysis. Our approach, GMM-MI, accurately models complex, multi-modal probability densities, enabling a precise decomposition of the total MI into contributions from distinct data regimes. We validate our estimator on synthetic data from turbulent flow simulations, demonstrating superior performance compared to existing methods. Applied to a variational autoencoder trained on Ising model configurations, GMM-MI successfully quantifies the emergence of latent variables corresponding to the system's order parameter near the critical temperature. This work establishes a principled framework for interpreting deep learning models in physics, directly linking informational-theoretic quantities to underlying physical phenomena.