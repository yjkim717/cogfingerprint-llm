**Abstract**

Recent advances in large language models (LLMs) have catalyzed progress across natural language processing. While specialized architectures for information extraction (IE) tasks like named entity recognition and relation extraction remain prevalent, their performance is often bounded by task-specific design and data availability. Concurrently, code-generation large language models (Code-LLMs) have demonstrated remarkable reasoning capabilities and structural precision. This paper investigates the hypothesis that formulating IE tasks as code-generation problems can leverage these strengths. We propose a novel methodology that frames IE as the generation of executable code snippets, which, when run, populate structured knowledge bases. Through extensive few-shot evaluations on standard benchmarks, we demonstrate that Code-LLMs significantly outperform both general-purpose natural language LLMs and state-of-the-art specialized IE models. Our analysis suggests that Code-LLMs excel at parsing complex linguistic constructs and enforcing strict output schemas through their inherent understanding of syntax and structure. These findings, established in 2023, challenge the conventional paradigm of task-specific IE systems and position code generation as a powerful, unified approach for structured information extraction.