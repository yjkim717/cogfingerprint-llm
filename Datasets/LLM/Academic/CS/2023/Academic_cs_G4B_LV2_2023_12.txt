**Abstract**

Recent advancements in Transformer architectures have driven significant progress across diverse domains, notably within computer vision. However, effective training remains a persistent challenge, often hampered by instability and protracted convergence times. This work investigates a novel weight initialization strategy for self-attention layers within Transformer models, termed “mimetic initialization.” We hypothesize that initializing these layers to mirror the weight distributions observed in pre-trained models—specifically, those trained on large-scale datasets—can substantially accelerate training and bolster performance. 

Experimental results, conducted across a suite of vision tasks in 2023, demonstrate a marked improvement in both training speed and final accuracy compared to conventional initialization methods.  Specifically, mimetic initialization reduces training epochs by an average of 15% while simultaneously enhancing validation performance. These findings suggest a critical pathway toward more efficient and robust Transformer training, highlighting the potential of leveraging pre-existing knowledge to bootstrap the learning process.