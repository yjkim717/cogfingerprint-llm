This paper investigates the representational dynamics of multimodal transformers through a granular analysis of attention mechanisms in vision-language models. We propose a methodology to decompose cross-modal representations across spatial, depth, and head dimensions, examining how visual patches interact with textual tokens throughout the network hierarchy. Our analysis reveals that specialized attention heads develop distinct functional roles in processing semantic relationships between image regions and linguistic concepts. We demonstrate how these interpretable components can be selectively enhanced or pruned to optimize model performance on downstream tasks. The findings provide new insights into the emergent properties of multimodal representations and establish principled approaches for architectural refinement in zero-shot visual understanding systems, with empirical validation across multiple benchmark datasets.