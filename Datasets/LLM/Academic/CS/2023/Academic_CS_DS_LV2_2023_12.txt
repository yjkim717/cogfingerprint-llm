This paper introduces Mimetic Initialization, a novel method for initializing self-attention layers in Transformer architectures for vision tasks. We posit that standard random initialization schemes create a suboptimal starting point, impeding training efficiency and final performance. Our approach directly addresses this by initializing layers to approximate the statistical properties and structural patterns observed in pre-trained, performant models, without requiring any learned parameters. Empirical evaluation on benchmark vision datasets demonstrates that models initialized with our method converge significantly faster and achieve higher accuracy compared to those using conventional initialization strategies. These findings suggest that a carefully designed, knowledge-free initialization can effectively bootstrap the training process, reducing computational costs and establishing a more robust foundation for learning in visual domains.