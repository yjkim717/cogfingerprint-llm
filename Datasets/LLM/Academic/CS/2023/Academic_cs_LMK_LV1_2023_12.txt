Title: Accelerating Vision Transformers via Mimetic Initialization of Self-Attention Layers

Abstract:
Recent advancements in computer vision have been driven by the adoption of Transformer architectures, which rely heavily on self-attention mechanisms. However, training these models from scratch can be computationally expensive and often requires large datasets. In this work, we propose a novel initialization strategy for self-attention layers in Vision Transformers (ViTs) that leverages pre-trained language Transformers. By initializing ViT self-attention layers to mimic the behavior of pre-trained Transformers, we demonstrate significant improvements in training speed and accuracy on various vision tasks. Our mimetic initialization approach enables the transfer of knowledge from language domains to vision tasks, bridging the gap between these two modalities. Experimental results on ImageNet and other benchmarks show that our method outperforms standard initialization techniques, achieving state-of-the-art performance while reducing training time. This work highlights the potential of knowledge transfer between domains via mimetic initialization.