**Abstract**

In 2023, the paradigm of Information Extraction (IE) is being reshaped by the application of code generation models, challenging the long-standing dominance of natural language (NL) models in tasks such as Named Entity Recognition (NER) and Relation Extraction (RE). This research investigates the hypothesis that by reformulating structured IE outputs as type-constrained code, we can leverage the inherent reasoning capabilities and data efficiency of large-scale code models. We present a systematic evaluation under a rigorous few-shot learning framework, demonstrating that this code-based formulation enables superior generalization and structural consistency compared to conventional sequence-to-sequence NL approaches. Our empirical analysis across seven diverse IE benchmarks confirms that code generation models achieve state-of-the-art performance, effectively mitigating common error types associated with free-text generation. These findings establish code-based IE as a transformative methodology, bridging the representational gap between unstructured text and structured knowledge, with significant implications for scalable and accurate knowledge base construction.