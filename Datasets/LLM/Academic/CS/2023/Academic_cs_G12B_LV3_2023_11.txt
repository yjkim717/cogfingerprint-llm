 Here's an academic abstract inspired by the provided summary and keywords, written in a formal style suitable for a CS publication and reflecting a 2023 context:

**Collision Cross-Entropy for Enhanced Deep Clustering with Soft Labels**

Deep clustering algorithms often struggle with ambiguous data points and noisy labels, particularly when employing soft label representations. This paper introduces Collision Cross-Entropy (CCE), a novel loss function designed to address these challenges. Unlike traditional Shannon cross-entropy, which inherently preserves label uncertainty, CCE explicitly promotes the alignment between predicted and ground truth class probabilities. By maximizing the likelihood of collision – where predicted and true labels agree – CCE encourages more decisive and accurate cluster assignments. We demonstrate that CCE effectively refines cluster quality and improves classification performance in deep clustering frameworks, particularly when training with self-labeling strategies and within an Expectation-Maximization (EM) paradigm. Empirical results indicate that CCE yields superior performance compared to standard cross-entropy losses, suggesting a promising avenue for advancing robust deep clustering methods.
