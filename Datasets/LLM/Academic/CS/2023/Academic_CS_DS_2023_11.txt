This paper investigates the capacity of neural networks to extrapolate beyond their training distribution when learning Boolean functions, a fundamental challenge in computational learning theory. We analyze how standard training procedures often fail to generalize to unseen function classes, particularly those with distinct spectral properties from the training set. Our theoretical framework connects this limitation to the concentration of Fourier mass across different frequency bands, revealing that networks tend to prioritize learning low-frequency components while struggling with higher-order interactions. Building on these insights, we propose a novel curriculum learning strategy that systematically exposes models to functions of increasing complexity, guided by spectral characteristics rather than random sampling. Through extensive experiments on synthetic Boolean tasks, we demonstrate that our spectral curriculum approach yields significant improvements in generalization accuracy compared to conventional training methods, particularly for functions with sparse high-frequency representations. These findings suggest that structured exposure to computational complexity, informed by spectral analysis, may provide a pathway toward more sample-efficient learning algorithms capable of broader generalization.