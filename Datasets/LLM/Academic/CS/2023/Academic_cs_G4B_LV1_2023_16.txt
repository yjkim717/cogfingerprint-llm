Here’s an academic abstract inspired by the provided summary, suitable for a 2023 computer science publication:

**Abstract**

The burgeoning demand for instantaneous speech-to-text applications necessitates robust, low-latency speech recognition systems. This paper presents Whisper-Streaming, an innovative architecture leveraging the pre-trained Whisper model to facilitate real-time transcription and translation with minimized delay.  Traditional Whisper deployments often introduce unacceptable latency due to batch processing.  Whisper-Streaming addresses this limitation through a streaming pipeline, employing a modified Whisper inference engine optimized for continuous input. We detail the system’s design, incorporating adaptive batching and prioritized processing to balance throughput and latency.  Experimental results demonstrate a significant reduction in end-to-end latency compared to standard Whisper implementations, while maintaining competitive transcription accuracy.  Furthermore, we analyze the trade-offs between latency reduction and computational resource utilization, highlighting the system’s scalability potential for diverse streaming applications, including live captioning and interactive voice interfaces.  Future work will explore further optimization and integration with multilingual translation services.