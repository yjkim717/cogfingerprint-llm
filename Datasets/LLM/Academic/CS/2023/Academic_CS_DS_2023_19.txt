This paper presents a methodological framework for integrating natural language feedback into iterative system refinement processes. We demonstrate how human-sourced textual critiques can systematically inform both evaluation metric formulation and instruction prompt optimization in complex AI systems. Our approach establishes a structured pipeline where qualitative feedback undergoes semantic analysis to identify performance gaps, which subsequently guide parametric adjustments in metric functions and prompt engineering strategies. Through three case studies involving dialogue systems and code generation models, we show that this human-in-the-loop methodology yields statistically significant improvements in both automated metrics and human evaluations compared to standard tuning approaches. The framework provides practitioners with formal mechanisms to translate subjective human judgments into actionable system modifications, bridging the gap between qualitative assessment and quantitative optimization in machine learning development cycles.