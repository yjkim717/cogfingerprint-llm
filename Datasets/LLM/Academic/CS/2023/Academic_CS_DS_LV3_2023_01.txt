**Abstract**  
This research presents a systematic deconstruction of the CLIP (Contrastive Language–Image Pre-training) vision transformer, focusing on interpretability mechanisms within its attention heads. By analyzing patch-wise contributions across transformer layers, we identify emergent spatial localization capabilities that underpin CLIP’s zero-shot reasoning. Our findings reveal that specific attention heads encode spurious features, which we mitigate through targeted ablation, enhancing model robustness and generalization. Furthermore, we demonstrate that these insights enable novel applications in zero-shot semantic segmentation without task-specific fine-tuning. This work not only advances the interpretability of multimodal architectures but also establishes a framework for improving transformer-based models by pruning redundant or misleading feature encodings. Our approach bridges architectural analysis with practical model refinement, offering pathways for more transparent and efficient vision-language systems.

**Keywords**: CLIP, transformer, interpretability, attention heads, zero-shot