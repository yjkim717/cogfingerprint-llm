Here’s an academic abstract inspired by the provided summary and keywords, suitable for a 2023 publication:

**Abstract**

Recent advances in large language models, particularly the CLIP architecture, have demonstrated remarkable zero-shot capabilities across diverse modalities. However, a fundamental understanding of CLIP’s internal image representation remains elusive. This research investigates the compositional nature of CLIP’s encoder, employing a novel text-based decomposition strategy to isolate and characterize distinct visual components. Specifically, targeted text prompts are utilized to elicit and analyze the influence of these internal representations on CLIP’s downstream performance. Our findings reveal that CLIP’s encoding process can be segmented into interpretable modules, each contributing uniquely to its overall image understanding.  Crucially, this analysis facilitates the development of targeted model repair techniques, addressing vulnerabilities identified through component-level examination. Furthermore, we demonstrate the efficacy of these decomposed representations in constructing a robust zero-shot image segmenter, achieving competitive performance against established methods while offering enhanced interpretability.