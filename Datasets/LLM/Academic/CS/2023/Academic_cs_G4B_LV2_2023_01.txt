**Abstract**

Recent advancements in CLIP (Contrastive Language-Image Pre-training) have demonstrated remarkable zero-shot image segmentation capabilities. However, the internal workings of CLIP’s image encoder remain largely opaque, hindering effective model diagnosis and targeted repair. This work introduces a novel approach to dissecting CLIP’s image representation through text-based decomposition. We leverage transformer models to systematically analyze the contribution of distinct visual components, identified via a process of textual interrogation. Our methodology reveals that CLIP’s encoder exhibits property-specific roles, with certain representations preferentially encoding attributes like texture, shape, and object boundaries. 

Specifically, we demonstrate how targeted textual prompts can isolate and modulate these internal representations, facilitating a more granular understanding of CLIP’s decision-making process.  This decomposition allows us to identify and mitigate biases within the encoder, ultimately contributing to the development of more robust and interpretable zero-shot image segmenters.  The findings presented here provide a foundational framework for future research into the inner workings of vision-language models and offer a pathway towards improved model reliability.