Title: Enhancing Few-Shot Information Extraction with Code-LLMs: A Comparative Analysis

Abstract:
The advent of large language models (LLMs) has revolutionized the field of natural language processing, particularly in few-shot learning scenarios. This study investigates the efficacy of code-LLMs, NL-LLMs, and fine-tuned models in few-shot information extraction (IE) tasks, with a focus on named entity recognition (NER). We conduct a comprehensive comparative analysis, evaluating the performance of these models on a range of IE tasks. Our results demonstrate that code-LLMs significantly outperform NL-LLMs and fine-tuned models in few-shot NER tasks, achieving state-of-the-art results. The superior performance of code-LLMs can be attributed to their ability to leverage structured code representations, enabling more effective knowledge transfer and adaptation to novel tasks. Our findings have important implications for the development of more efficient and accurate IE systems, highlighting the potential of code-LLMs to drive advancements in this field. The results underscore the importance of exploring code-LLMs for IE tasks in low-resource settings.