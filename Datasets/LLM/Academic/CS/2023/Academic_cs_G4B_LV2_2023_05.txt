**Abstract**

Subjective evaluations remain a critical bottleneck in the development of robust hate speech detection systems, particularly when considering diverse target groups. This paper introduces a novel predictive model designed to quantify and mitigate annotator disagreement within these evaluations. Leveraging demographic information associated with individual annotators, our approach aims to forecast inter-annotator variance in subjective ratings—specifically, judgments of hate speech—before deployment. Central to our methodology is the incorporation of model uncertainty estimation, allowing us to identify instances where annotator opinions diverge significantly.  We hypothesize that systematic biases related to target group affiliation contribute substantially to observed disagreement.  

Experimental results, presented in 2023, demonstrate the model’s efficacy in predicting variance and highlight the correlation between demographic characteristics and annotator divergence.  Furthermore, we showcase how quantifying model uncertainty can inform post-processing strategies, such as consensus scoring, thereby enhancing the reliability of hate speech detection models.  Ultimately, this work contributes to a more nuanced understanding of subjective evaluation challenges and offers a pathway towards more trustworthy and equitable automated content moderation.