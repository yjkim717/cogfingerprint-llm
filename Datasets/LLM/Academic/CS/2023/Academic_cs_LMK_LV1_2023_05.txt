Title: Quantifying Uncertainty in Hate Speech Detection through Annotator Disagreement Modeling

Abstract:
Hate speech detection, a subjective task in natural language processing (NLP), is plagued by annotator disagreement, which can significantly impact model performance and reliability. To address this challenge, we propose a novel framework that predicts annotator ratings for potentially offensive text, thereby capturing the nuances of human judgment. By modeling annotator disagreement, our approach not only improves the performance of hate speech detection models but also provides a quantifiable metric for model uncertainty. We leverage a probabilistic modeling paradigm to estimate the distribution of annotator ratings, allowing us to capture the inherent subjectivity in hate speech annotation. Our experiments demonstrate that the proposed framework achieves state-of-the-art performance on benchmark datasets while providing a robust measure of model uncertainty. The results underscore the importance of accounting for annotator disagreement in hate speech detection tasks, paving the way for more reliable and informative NLP models in 2023 and beyond.