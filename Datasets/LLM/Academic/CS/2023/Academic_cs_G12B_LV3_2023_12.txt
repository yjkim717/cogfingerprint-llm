 Here's an academic abstract inspired by the provided summary and keywords, written in a formal style and adhering to the specified constraints:

**Mimetic Initialization for Accelerated Transformer Training in Vision Tasks**

Recent advancements in Transformer architectures have demonstrated remarkable capabilities across various domains, yet their training demands remain a significant bottleneck, particularly within computer vision. This paper introduces mimetic initialization, a learning-free pre-training strategy designed to mitigate these challenges. Our approach leverages the knowledge embedded within established, pre-trained Transformer models to inform the initialization of subsequent layers. Specifically, we align the weight matrices of self-attention mechanisms with observed patterns and statistical properties derived from these pre-trained networks. Empirical evaluation across several vision tasks demonstrates that mimetic initialization substantially accelerates training convergence and improves overall model accuracy without requiring task-specific fine-tuning during the initialization phase. These findings suggest that mimetic initialization offers a practical and efficient pathway to harness the benefits of large-scale pre-training while reducing the computational burden associated with training Transformers from scratch.



