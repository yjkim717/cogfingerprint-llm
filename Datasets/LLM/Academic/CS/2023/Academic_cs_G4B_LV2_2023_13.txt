Here’s an academic abstract based on the provided summary, aiming for approximately 174 words and reflecting a 2023 research context:

**Abstract**

Recent advancements in neural network architectures have highlighted the persistent challenge of systematic generalization – the ability to rapidly adapt to unseen compositional tasks. This paper introduces a meta-learning framework designed to address this limitation, demonstrating a pathway toward human-like performance in compositional learning scenarios. Our approach leverages meta-training to cultivate intrinsic skills that facilitate systematic generalization, moving beyond task-specific optimization. Specifically, we explore how meta-learning can be systematically coupled with instruction learning, allowing networks to acquire reusable compositional primitives. 

The core innovation lies in optimizing for a latent space representing these compositional skills, thereby promoting transferability across diverse, yet structurally related, tasks. Empirical evaluations across a suite of benchmark datasets reveal that our method consistently outperforms conventional fine-tuning strategies, exhibiting significantly improved performance in zero-shot generalization. These results underscore the potential of meta-learning to unlock more robust and adaptable neural networks, representing a crucial step towards artificial intelligence systems capable of truly understanding and generalizing from experience.