In 2023, the persistent challenge of noisy and ambiguous class assignments in unsupervised deep learning has prompted a critical re-evaluation of foundational loss functions. This paper introduces Collision Cross-Entropy (CCE), a novel information-theoretic objective designed to supersede Shannon's cross-entropy in contexts involving probabilistic or "soft" labels. While conventional cross-entropy can be unstable with non-categorical targets, CCE provides a robust theoretical framework that explicitly models the "collision" probability of label distributions, thereby enhancing resilience to label uncertainty. We integrate CCE into two prominent paradigms: discriminative deep clustering and self-labeling via the Expectation-Maximization (EM) algorithm. In deep clustering, CCE acts as a stable regularizer, mitigating degenerate solutions by effectively utilizing the soft assignment distributions generated by the network. For self-labeling methods, which refine labels and model parameters iteratively, CCE's properties ensure more consistent and reliable convergence of the EM steps, as it is less susceptible to the confirmation bias inherent in noisy pseudo-label updates. Extensive empirical evaluations on benchmark vision and text datasets demonstrate that our approach achieves state-of-the-art clustering accuracy and purity. By addressing the core instability in handling soft labels, this work establishes CCE as a fundamental component for the next generation of unsupervised and self-supervised learning systems, paving the way for more trustworthy and scalable representation learning.