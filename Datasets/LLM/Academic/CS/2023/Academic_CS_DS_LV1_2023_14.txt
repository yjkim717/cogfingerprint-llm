**Abstract**

The advancement of automated vehicle (AV) technology necessitates robust multi-agent cooperation frameworks capable of ensuring safety in dense, dynamic traffic scenarios. While existing research has extensively explored collision avoidance for individual vehicles, strategies for coordinated decision-making among multiple AVs remain fragmented, often relying on pre-defined rules with limited scalability. This paper posits that an integrated, AI-enabled architecture is critical to overcome these limitations. We propose a novel hierarchical framework that synergizes deep reinforcement learning (DRL) for high-level cooperative strategy formulation with real-time distributed optimization for low-level trajectory control. The DRL module is trained in a simulated multi-agent environment to learn policies that optimize global traffic flow and preemptively resolve complex conflict scenarios, such as unsignalized intersections and lane merges. These high-level intentions are then executed by a distributed model predictive control layer, which ensures collision-free trajectories under vehicle dynamics and sensing uncertainty constraints. Simulation results demonstrate that our approach reduces average travel time by 18% and eliminates deadlock scenarios in high-density traffic compared to baseline decentralized models. This work underscores the pivotal role of integrating strategic AI planning with precise distributed control to achieve safe and efficient large-scale vehicle cooperation.