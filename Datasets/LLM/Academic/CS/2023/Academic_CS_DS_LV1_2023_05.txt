**Abstract**

The inherent subjectivity of natural language processing (NLP) tasks, such as hate speech detection, is reflected in significant annotator disagreement, which is often treated as noise. This paper posits that such disagreement is a valuable signal, encapsulating diverse perspectives shaped by annotator demographics and lived experiences. We introduce a privacy-preserving, multi-annotator modeling framework that moves beyond predicting aggregated labels. Our method leverages non-identifiable demographic covariates and generalized online experience metrics to directly predict individual annotator ratings on contentious text samples. We demonstrate that incorporating this granular perspective significantly improves the prediction of the distribution of human judgments compared to models trained on majority-vote labels. Furthermore, we analyze the calibrated uncertainty of these models, showing that high predictive uncertainty often aligns with samples where target group opinions diverge, providing a nuanced, human-aligned measure of classification difficulty. This approach not only enhances model performance on subjective tasks but also offers a pathway to developing AI systems that are more cognizant of societal context and the plurality of human values, a critical consideration for equitable NLP in 2023.