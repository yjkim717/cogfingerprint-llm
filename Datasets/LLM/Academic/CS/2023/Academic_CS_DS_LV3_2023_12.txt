In 2023, the computational overhead of training vision transformers from scratch remains a significant bottleneck, particularly for resource-constrained environments. This paper introduces mimetic initialization, a novel, closed-form method for initializing self-attention layers in vanilla Transformer architectures. Unlike conventional pre-trained models or learned initialization schemes, our technique deterministically sets the products of the query-key and value-projection weights to approximate identity and negative identity mappings, respectively. This strategic initialization provides a robust inductive bias, effectively stabilizing the early stages of training by preserving input signal integrity and mitigating gradient pathologies. Empirical evaluations on standard vision benchmarks, including CIFAR-10 and ImageNet, demonstrate that models initialized with our method achieve competitive top-1 accuracy significantly faster than baseline Transformers, reducing the required training epochs by up to 40%. These findings suggest that carefully designed, learning-free initialization protocols can substantially enhance the efficiency and performance of foundational models, offering a compelling alternative to large-scale pre-training for vision tasks.