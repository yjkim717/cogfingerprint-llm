## Meta-Learning for Compositional Generalization: Towards Systematically Grounded Neural Networks

**Abstract:** The ability to generalize systematically – leveraging learned components to construct novel and meaningful combinations – remains a significant challenge for neural network models. While deep learning has achieved remarkable performance on specific tasks, exhibiting limited capacity for systematicity comparable to human cognition, recent research explores meta-learning approaches to address this deficiency. This work investigates a novel framework, termed Compositional Skill Learning (CSL), extending the principles of meta-learning for compositionality (MLC) to enhance systematic generalization in neural networks. CSL focuses on optimizing a meta-learner to acquire a diverse set of reusable skills – modular, functionally cohesive units – and a compositional mechanism capable of combining these skills in a logically consistent manner. Crucially, we introduce a novel training regime incorporating explicitly defined compositional constraints, promoting the emergence of systematicity during both skill acquisition and combination. Empirical evaluation across a range of symbolic and continuous compositional tasks demonstrates that CSL significantly improves generalization performance on unseen combinations compared to established baselines, including standard neural network architectures and prior MLC implementations. Furthermore, analysis of the learned skills reveals a degree of modularity and functional specialization akin to observed cognitive structures. This research suggests that meta-learning, coupled with explicitly guided compositional constraints, provides a promising avenue for developing neural networks exhibiting human-like systematic generalization capabilities, advancing the field towards more robust and adaptable artificial intelligence systems in 2023.



