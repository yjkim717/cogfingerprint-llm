
## Collision Cross-Entropy: A Distribution-Aware Loss Function for Enhanced Deep Clustering

Deep clustering methods, while demonstrating promise in unsupervised learning, often struggle with the inherent ambiguity of data and the limitations of hard assignment strategies. Traditional loss functions frequently penalize incorrect assignments without adequately considering the nuanced relationship between predicted and true class distributions. This paper proposes Collision Cross-Entropy (CCE), a novel loss function designed to address this limitation within the context of deep clustering. CCE moves beyond point-wise accuracy by explicitly modeling the probability of “collision” – the overlap between the predicted and ground truth class distributions represented by soft labels.  

Our approach frames clustering as a distribution matching problem, minimizing the cross-entropy between these distributions rather than solely focusing on assignment accuracy.  The core innovation lies in incorporating a collision probability term into the cross-entropy calculation, penalizing predictions that significantly overlap with incorrect classes while rewarding assignments that closely align with the true distribution.  We argue that this encourages the network to learn more discriminative features and produce assignments reflecting the underlying data structure, even with inherent ambiguities.  

Experiments on benchmark datasets demonstrate that CCE consistently outperforms state-of-the-art deep clustering algorithms, including those employing self-labeling strategies, across various performance metrics such as Normalized Mutual Information (NMI) and Adjusted Rand Index (ARI). We further analyze the impact of the collision probability parameter, revealing its sensitivity to dataset complexity. This work contributes a theoretically grounded and empirically validated loss function aiming to advance the field of deep clustering and improve the robustness of unsupervised learning models.
