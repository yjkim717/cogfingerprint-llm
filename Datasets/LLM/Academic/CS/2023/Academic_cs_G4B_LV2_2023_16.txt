**Abstract**

The burgeoning field of automatic speech recognition (ASR) demands solutions capable of processing audio in near real-time, particularly for applications such as live translation and interactive voice systems. This paper presents Whisper-Streaming, a novel system architecture leveraging the robust Whisper model to facilitate low-latency, high-fidelity streaming transcription.  We detail the implementation of Whisper’s inference pipeline, incorporating dynamic batching and optimized memory management to minimize processing delays.  Crucially, Whisper-Streaming demonstrates efficacy in multilingual scenarios, retaining the model’s inherent support for numerous languages during continuous audio input.  Experimental results indicate a significant reduction in latency compared to traditional, non-streaming Whisper deployments, while maintaining competitive transcription accuracy.  We evaluate the system’s performance under varying network conditions and audio complexities, suggesting potential for deployment in resource-constrained environments. Future work will explore adaptive streaming strategies and enhanced integration with translation services.