Of course. Here is a formal academic abstract inspired by your provided summary and keywords, crafted for the computer science domain in the 2023 context.

***

**Abstract**

A central challenge in artificial intelligence is the gap between the impressive in-domain performance of neural networks and their frequent failure to exhibit systematic generalizationâ€”a hallmark of human cognition where learned skills are composed and applied to novel situations. This paper posits that the predominant paradigm of training on static, large-scale datasets is fundamentally misaligned with the development of robust compositional reasoning. We investigate the "Meta-Learning for Compositionality" (MLC) framework as a principled solution to this limitation. Unlike standard methods, MLC explicitly optimizes models for compositional skills by exposing them to a dynamic stream of tasks, where the training objective itself is to learn how to systematically compose known primitives. We present a series of empirical studies in visual and linguistic reasoning domains, demonstrating that networks trained under the MLC framework achieve near-perfect generalization on held-out compositional tests, significantly outperforming conventional supervised learning and other meta-learning baselines. Our analysis reveals that MLC induces internal representations that are more modular and algebraically structured, facilitating the re-combination of functional components. These findings suggest that the meta-learning objective directly shapes the network's inductive biases towards compositionality. Consequently, this work provides a compelling bridge between connectionist models and symbolic cognitive theories, arguing that dynamic, task-oriented training curricula are essential for building artificial systems that capture the systematicity and flexibility of human thought. The MLC approach establishes a new benchmark for evaluating and achieving human-like generalization in machine learning.