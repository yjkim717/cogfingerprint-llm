Title: Leveraging Code-LLMs for Enhanced Few-Shot Information Extraction

Abstract:
The advent of large language models (LLMs) has revolutionized the field of natural language processing, with recent advancements extending to code-centric models, termed Code-LLMs. This study investigates the efficacy of Code-LLMs in few-shot information extraction (IE) tasks, comparing their performance against traditional natural language LLMs (NL-LLMs) and fine-tuned models. Our results demonstrate that Code-LLMs outperform both NL-LLMs and fine-tuned models in few-shot IE tasks, showcasing their superior capability in extracting relevant information from limited annotated data. We attribute this performance disparity to the inherent structural understanding and generation capabilities of Code-LLMs, which enable them to effectively leverage the contextual information provided in few-shot learning scenarios. The findings of this research underscore the potential of Code-LLMs as a promising direction for advancing IE tasks, particularly in data-scarce environments. Our work contributes to the ongoing exploration of LLMs in IE and highlights the benefits of integrating code-centric approaches in NLP tasks.