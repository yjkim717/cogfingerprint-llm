Title: Decomposing CLIP's Image Representation: A Text-Based Analysis for Enhanced Zero-Shot Performance

Abstract:
Contrastive Language-Image Pre-training (CLIP) has revolutionized the field of computer vision with its remarkable zero-shot capabilities. However, the underlying components of its image representation remain poorly understood. This study employs a text-based decomposition approach to analyze CLIP's image representation, shedding light on its constituent parts. By leveraging transformer models, we decompose CLIP's image embeddings into text-based attributes, facilitating a more nuanced understanding of its representation space. Our analysis reveals key insights into the semantic structure of CLIP's image representation, enabling the identification of areas for improvement. We demonstrate that this decomposition can be utilized to enhance CLIP's zero-shot performance, achieving state-of-the-art results on benchmark datasets. This research contributes to the ongoing effort to interpret and improve transformer-based vision models, paving the way for more accurate and robust image-text representation learning.