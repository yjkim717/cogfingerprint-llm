**Abstract**

This work presents a granular analysis of the internal representations within the Contrastive Language-Image Pre-training (CLIP) image encoder. We systematically deconstruct the final embedding by examining contributions from individual image patches across the network's layers and multi-head self-attention mechanisms. Our methodology reveals that specific attention heads function as specialized feature detectors, aligning visual patterns with semantic text concepts embedded in the class prompts. This interpretability framework enables us to identify and prune redundant heads with minimal performance loss. Consequently, we propose a targeted fine-tuning strategy that enhances the model's efficacy on zero-shot segmentation tasks by reinforcing the most salient attention pathways. Our findings demonstrate that a mechanistic understanding of representation formation is critical for optimizing and adapting large-scale vision-language models for downstream applications.