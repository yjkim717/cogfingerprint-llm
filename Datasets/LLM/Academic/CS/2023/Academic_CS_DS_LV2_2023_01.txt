This study presents a systematic analysis of CLIP's image encoder through hierarchical decomposition across spatial patches, transformer layers, and attention heads. We develop an interpretability framework that quantifies component contributions to zero-shot image segmentation performance. Our methodology reveals that specific attention heads specialize in processing distinct visual patterns, with spatial localization emerging primarily in middle layers while semantic integration occurs in deeper layers. By selectively pruning underperforming components, we demonstrate a 15% improvement in segmentation accuracy while reducing computational overhead. These findings establish that CLIP's representational efficacy stems from specialized functional分工 rather than uniform processing, providing both interpretability insights and practical optimization pathways for vision-language models. This work advances model understanding beyond aggregate performance metrics toward component-level architectural analysis.