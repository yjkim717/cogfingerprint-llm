**Abstract**

The efficacy of Vision Transformers (ViTs) is critically dependent on their initialization, as random schemes often lead to unstable optimization and protracted convergence. In this work, we introduce Mimetic Initialization, a principled, closed-form method for initializing self-attention layers. Our approach analytically derives initial weights that emulate the statistical properties and structural patterns of performant, pre-trained attention modules. By constructing initial query, key, and value projection matrices that induce a stable, content-aware feature aggregation from the outset, we circumvent the need for extensive pre-training or sophisticated warm-up strategies. Empirical evaluation on ImageNet classification demonstrates that models initialized with our method achieve superior accuracy (e.g., +1.8% for ViT-Tiny) and significantly accelerated convergence, requiring up to 40% fewer training epochs to match the performance of standard baselines. This work establishes that a theoretically-grounded initialization can substantially enhance the training dynamics and final performance of transformer-based architectures in computer vision.

*(Year: 2023)*