**Abstract**

Recent advancements in large language models (LLMs) have sparked considerable interest in their applicability across diverse natural language processing (NLP) domains. This study investigates the efficacy of Code-LLMs, specifically leveraging the capabilities of models such as Codex, for information extraction (IE) tasks. We hypothesize that prompting these models with code-style instructions significantly enhances their performance relative to traditional Natural Language LLMs (NL-LLMs) in IE scenarios. Through controlled experiments focusing on named entity recognition (NER) and relation extraction (RE), we demonstrate a marked improvement in accuracy and efficiency when utilizing code generation techniques. Our findings suggest that Code-LLMsâ€™ inherent capacity for structured output and logical reasoning, facilitated by their training on code, positions them as a superior approach for complex IE pipelines.  This work contributes to a growing body of research exploring the potential of hybrid LLM architectures and highlights a promising avenue for optimizing IE systems in 2023 and beyond.