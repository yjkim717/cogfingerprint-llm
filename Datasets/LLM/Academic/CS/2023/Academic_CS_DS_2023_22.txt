This paper presents a streaming framework for neural speech recognition that achieves sub-4 second latency while maintaining multilingual transcription capabilities. Our architecture employs adaptive buffering and incremental decoding to process continuous audio streams, addressing the temporal alignment challenges inherent in real-time systems. Evaluations across six languages demonstrate robust performance in dynamic conference scenarios with overlapping speakers. The system bridges the gap between batch-processing models and practical applications requiring immediate transcription, establishing a new benchmark for deployable low-latency speech-to-text systems without sacrificing accuracy.