**Abstract**

The inherent subjectivity of tasks such as hate speech detection manifests as significant annotator disagreement, which is often treated as noise in model training. This work posits that such disagreement is a valuable signal, reflecting the diverse perspectives of different demographic groups. We propose a novel modeling framework that explicitly incorporates anonymized demographic information of annotators to predict their individual ratings, rather than aggregating labels to a single ground truth. This approach allows a model to capture the nuanced criteria used by various demographic segments, thereby better approximating the opinion distribution of a target population. A key contribution is our investigation into the relationship between a model's predictive uncertainty and demographic-driven disagreement, demonstrating that uncertainty estimates can flag contentious instances requiring deeper sociolinguistic analysis. Furthermore, we address the critical challenge of privacy preservation by evaluating methods to utilize demographic data without compromising annotator anonymity. Our empirical results on hate speech classification datasets indicate that models leveraging this disaggregated, demographic-aware strategy achieve superior alignment with subgroup judgments and provide more transparent uncertainty quantification, advancing the development of equitable and context-sensitive NLP systems.

(199 words)