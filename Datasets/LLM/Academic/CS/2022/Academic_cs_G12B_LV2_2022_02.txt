 Here's a formal academic-style abstract, suitable for a CS conference or journal, based on the provided keywords and summary, and adhering to a 2022 timeframe:

**Abstract**

The problem of learning halfspaces from data corrupted by noise has long been a central focus in machine learning, with significant attention devoted to the analysis of various noise models. This paper investigates the computational hardness of learning halfspaces under the Massart noise model, a form of sub-Gaussian noise that has seen increasing relevance in contemporary applications.  We demonstrate a strong connection between this learning problem and the hardness of the Learning With Errors (LWE) problem, a foundational problem in post-quantum cryptography. Specifically, we prove that learning a halfspace with Massart noise from *n* data points requires time at least polynomial in the sample size *n* under the assumption that LWE is computationally intractable. Our reduction leverages a novel transformation that maps instances of the halfspace learning problem with Massart noise to instances of LWE, demonstrating a concrete link between the statistical learning and cryptographic hardness regimes. This result establishes a fundamental limit on the efficiency of polynomial-time algorithms for this specific learning task and provides a rigorous framework for understanding the inherent computational complexity of learning in the presence of Massart noise. Furthermore, it offers a potential avenue for designing cryptographic primitives based on the difficulty of robust classification problems.




