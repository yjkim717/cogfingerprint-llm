This paper introduces a novel framework for automated text quality assessment by integrating discriminative and generative evaluation paradigms. While discriminative metrics excel at identifying fine-grained textual errors through classification tasks, generative approaches capture holistic semantic coherence by reconstructing perturbed inputs. Our proposed method, UniScore, unifies these complementary perspectives through multi-task learning on a transformer architecture. The model simultaneously optimizes for discriminative text ranking and generative text infilling objectives during pre-training, followed by task-specific fine-tuning. Evaluations conducted in 2022 demonstrate that UniScore significantly outperforms existing metrics across diverse benchmarks including text simplification, grammatical error correction, and style transfer tasks in multiple languages. The unified representation learning approach shows particular strength in cross-domain generalization, achieving average improvements of 3.7 points in correlation with human judgments compared to previous state-of-the-art methods. These results establish that hybrid evaluation frameworks leveraging both discriminative and generative signals provide more robust and comprehensive text quality assessment than either approach in isolation.