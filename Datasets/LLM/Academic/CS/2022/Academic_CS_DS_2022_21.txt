This paper investigates decoding strategies for neural machine translation that explicitly incorporate quality estimation during inference. We propose a family of methods that extend minimum Bayes risk decoding by integrating automated metrics as utility functions during beam search and n-best list reranking. Our approach dynamically adjusts candidate generation based on predicted translation quality, moving beyond traditional maximum a posteriori decoding. Experimental results across multiple language pairs demonstrate that quality-informed decoding strategies consistently improve translation performance according to both automatic metrics and human evaluation. The findings suggest that explicit quality modeling during decoding provides more reliable translation outputs than sequence likelihood optimization alone, offering new directions for robust machine translation systems.