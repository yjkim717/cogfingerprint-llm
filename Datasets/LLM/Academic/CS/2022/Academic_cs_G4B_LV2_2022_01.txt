Here’s an academic-style abstract based on the provided summary, suitable for a 2022 publication in computer science:

**Abstract**

Siamese representation learning, particularly within the framework of non-contrastive Self-Supervised Learning (SSL), has demonstrated significant promise for transfer learning. However, the phenomenon of dimensional collapse – a reduction in representation dimensionality – remains a critical, yet incompletely understood, factor influencing downstream task performance. This work investigates the nuanced relationship between dataset scale and dimensional collapse within Siamese networks, specifically employing the Simsiam objective. We introduce a novel collapse metric, predicated on analyzing the variance distribution within the learned representations, to quantitatively predict a model’s efficacy on diverse downstream benchmarks. Our analysis reveals a non-linear dependence, highlighting that smaller datasets can induce more pronounced collapse, potentially hindering effective knowledge transfer.  Furthermore, we demonstrate that this collapse metric serves as a valuable diagnostic tool, allowing researchers to proactively mitigate the detrimental effects of dimensionality reduction and improve the robustness of non-contrastive SSL techniques.  Future work will explore adaptive strategies for managing collapse across varying data regimes, contributing to more reliable continual learning systems.