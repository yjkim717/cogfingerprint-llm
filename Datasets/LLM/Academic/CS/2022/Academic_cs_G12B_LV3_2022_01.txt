 Here's an academic-style abstract inspired by the provided summary and keywords, suitable for a CS publication in 2022:

**Abstract**

Non-contrastive self-supervised representation learning, exemplified by methods like SimSiam, has emerged as a promising paradigm for learning robust feature spaces without reliance on negative samples. However, these approaches are susceptible to a critical failure mode: dimensional collapse, wherein the learned representations converge to a trivial solution. This work investigates the prevalence and characteristics of dimensional collapse within non-contrastive Siamese networks, revealing a strong correlation with both dataset scale and model capacity. We introduce a novel quantitative metric for forecasting collapse propensity, enabling proactive identification of vulnerable architectures. Furthermore, our findings demonstrate that incorporating continual learning strategies effectively mitigates collapse, facilitating the extraction of more discriminative and generalizable features. Empirical evaluations across various downstream tasks confirm that continual learning not only prevents collapse but also leads to substantial improvements in accuracy, highlighting a potential pathway for enhancing the reliability and practical utility of non-contrastive self-supervised learning.