Title: Assessing Dynamic Benchmarking in Machine Learning: A Theoretical Framework

Abstract:
The proliferation of machine learning models has underscored the need for robust evaluation methodologies. Dynamic benchmarking, which involves continuous data collection and model reassessment, has emerged as a promising approach to addressing the limitations of traditional benchmarking methods. This study (2022) provides a theoretical foundation for dynamic benchmarking by examining its efficacy through two distinct models of data collection and model fitting. We investigate the impact of label noise and hierarchical dependency structures on the performance of dynamically updated benchmarks. Our analysis reveals that dynamic benchmarking can mitigate the effects of label noise, while also capturing complex dependency structures. However, our results also highlight potential limitations, including the risk of overfitting and the need for careful tuning of hyperparameters. Our findings contribute to a deeper understanding of the benefits and challenges of dynamic benchmarking, informing the development of more effective evaluation methodologies in machine learning.