This paper investigates the phenomenon of dimensional collapse in self-supervised representation learning, with a focus on non-contrastive Siamese architectures. We identify that feature redundancy and subspace dominance are primary failure modes, leading to suboptimal representations despite preventing complete representational collapse. To quantify this, we introduce two novel metrics: the Effective Rank Ratio and the Subspace Invariance Score, which measure the diversity and stability of learned embeddings. Our analysis reveals that these architectures implicitly perform progressive feature filtering, which can prematurely discard useful information. We demonstrate that integrating principles from continual learning—specifically, through dynamic regularization that preserves feature plasticity—mitigates dimensional collapse without requiring negative samples. Empirical evaluations on ImageNet and CIFAR-100 show that our approach improves linear evaluation accuracy by 2.1–3.7% and enhances transfer learning performance across diverse downstream tasks, establishing a new connection between representation learning and continual learning paradigms.