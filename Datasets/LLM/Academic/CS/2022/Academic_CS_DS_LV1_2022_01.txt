**Abstract**

While non-contrastive Siamese networks (e.g., SimSiam) have demonstrated remarkable success in self-supervised representation learning, their performance is critically contingent on avoiding dimensional collapse, a phenomenon where embeddings occupy a low-dimensional subspace. This work investigates the systemic factors governing this stability. We empirically establish a strong correlation between dataset scale, model capacity, and the onset of collapse, demonstrating that larger architectures trained on smaller datasets are disproportionately susceptible. To address the prohibitive cost of full fine-tuning for performance estimation, we introduce two novel, training-free metrics: **Representational Rank Stability (RRS)**, which quantifies the effective dimensionality of the learned manifold, and **Augmentation Invariance Divergence (AID)**, which measures the consistency of representations under stochastic augmentations. Our findings, validated across multiple vision benchmarks, indicate that these metrics serve as robust early indicators of downstream task performance, providing a practical framework for architecture and hyperparameter selection in resource-constrained continual learning scenarios. This analysis delineates the preconditions for effective non-contrastive learning and offers scalable tools for its reliable deployment.

(178 words)