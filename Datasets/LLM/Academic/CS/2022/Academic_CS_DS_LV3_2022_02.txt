In this work, we establish a fundamental computational barrier for efficiently learning halfspaces in the challenging Massart noise model. Despite significant progress in designing polynomial-time algorithms that achieve error rates of η + ε for arbitrarily small ε, the question of whether one can reach the information-theoretic limit of η has remained a central open problem. We resolve this by proving a strong computational lower bound: under the widely believed subexponential-time hardness of the Learning with Errors (LWE) problem, no polynomial-time algorithm can PAC learn halfspaces with error better than Ω(η). Our reduction provides a rigorous complexity-theoretic explanation for the observed statistical-computational gaps in this setting. This result implies that achieving the optimal error rate is computationally intractable, thereby aligning the computational landscape of Massart halfspaces with other learning problems where such gaps are provably inherent. Our findings underscore the intrinsic difficulty of robust learning even in the presence of bounded, adversarial-label noise.