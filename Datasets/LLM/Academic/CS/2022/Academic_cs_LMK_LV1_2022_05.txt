Title: Revisiting Dynamic Benchmarking: A Theoretical Examination of Model Fitting and Label Noise in Hierarchical Dependency Settings

Abstract:
Dynamic benchmarking has emerged as a promising paradigm for evaluating the performance of machine learning models in real-world scenarios. In this paper, we present a theoretical investigation of dynamic benchmarking, focusing on its efficacy in capturing model fitting dynamics and mitigating label noise in datasets with hierarchical dependencies. We introduce two novel models, namely the Hierarchical Dependency Model (HDM) and the Label Noise Model (LNM), to analyze the benefits and limitations of dynamic benchmarking. Our theoretical analysis, supplemented by simulations on benchmark datasets such as CIFAR-10 and ImageNet, reveals that dynamic benchmarking can effectively capture the dynamics of model fitting in the presence of hierarchical dependencies. However, our results also indicate that label noise can significantly impact the reliability of dynamic benchmarking. We discuss the implications of our findings for the design of robust dynamic benchmarking protocols in 2022 and beyond.