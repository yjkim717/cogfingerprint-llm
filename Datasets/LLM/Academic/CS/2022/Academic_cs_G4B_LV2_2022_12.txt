**Abstract**

Recent advancements in computer vision have largely attributed the success of Vision Transformers (ViTs) to the efficacy of their inherent patch embedding strategy. However, the underlying architectural contributions of ViTs remain a subject of ongoing debate. This work addresses this ambiguity by systematically evaluating the relative importance of architectural innovation versus patch embedding design in achieving superior performance. We introduce ConvMixer, a novel convolutional architecture that leverages a modified MixConv block, eschewing the self-attention mechanisms prevalent in ViTs. Through extensive experimentation on the ImageNet dataset, ConvMixer demonstrates significant improvements over baseline ViT models, achieving state-of-the-art accuracy with substantially reduced computational complexity.  

Our analysis reveals that ConvMixerâ€™s success is primarily attributable to its convolutional structure, suggesting that efficient feature extraction via local receptive fields, combined with a carefully designed mixing strategy, can effectively replicate and surpass the performance gains observed in ViTs.  These findings offer valuable insights into the design principles of modern convolutional networks and highlight the potential of hybrid architectures that integrate convolutional and transformer elements.  The results, presented in 2022, contribute to a more nuanced understanding of the factors driving recent progress in visual representation learning.