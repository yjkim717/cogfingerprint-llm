This paper establishes the first computational hardness result for PAC learning homogeneous halfspaces under the Massart noise model. While information-theoretic bounds permit efficient algorithms achieving error η + ε, we demonstrate that computational barriers emerge under cryptographic assumptions. Specifically, we prove that no polynomial-time algorithm can output a hypothesis with error better than Ω(η) for any constant η > 0, assuming the hardness of the Learning With Errors (LWE) problem. Our reduction transforms LWE instances into carefully constructed Massart noise distributions where any sufficiently accurate halfspace learner would implicitly solve the underlying lattice problem. This work provides theoretical justification for the observed performance gap between practical Massart noise learners and their information-theoretic limits, positioning Massart noise as a natural analogue to the agnostic model within the distribution-specific PAC framework. Our results suggest that achieving optimal error in polynomial time may require exploiting additional structural properties beyond the standard Massart condition.