This paper investigates the pervasive issue of representation collapse in non-contrastive self-supervised learning frameworks. While these methods, particularly Siamese networks, circumvent the need for negative pairs, they remain susceptible to dimensional and complete collapse, where the learned representations fail to capture the full data variance. We first introduce a novel, computationally efficient metric to quantify the severity of this collapse phenomenon. Our central contribution is the theoretical and empirical demonstration that a continual learning paradigm inherently mitigates collapse. By training the network on a sequence of non-stationary tasks, the model is prevented from converging to trivial, collapsed solutions. Extensive experiments confirm that this approach not only maintains representation diversity but also yields significant performance gains on downstream tasks. Our findings position continual learning not merely as a solution to catastrophic forgetting but as a critical, regularizing component for robust representation learning, offering a new perspective on stabilizing non-contrastive methods without architectural modifications.