**Abstract**

The increasing accessibility of language models as a service (LMaaS) necessitates efficient methods for prompt optimization. Traditional techniques, reliant on gradient descent, are ill-suited for the inherently black-box nature of these models. This paper presents a novel black-box tuning framework designed to overcome this limitation, specifically targeting prompt engineering for RoBERTa-based language models. We demonstrate that this approach, leveraging derivative-free optimization strategies, significantly outperforms both manual prompt design and conventional gradient-based methods.  Our methodology focuses on systematically exploring the prompt space without requiring gradient information, effectively mitigating the challenges associated with LMaaS deployments.  Preliminary results indicate a notable improvement in model performance, suggesting a viable alternative for maximizing utility within constrained resource environments. Further research will investigate scaling and robustness of this technique across diverse LMaaS platforms and model architectures.