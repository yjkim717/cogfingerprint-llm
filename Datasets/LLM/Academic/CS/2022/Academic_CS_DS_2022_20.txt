This study investigates the viability of large language models (LLMs) as automated data annotation systems for natural language processing. We conduct a comparative analysis of annotations generated by a state-of-the-art LLM against a gold-standard human-annotated corpus across multiple classification tasks. Our methodology assesses both raw zero-shot performance and the model's output after minimal prompt-based calibration. Results indicate that while LLMs achieve significant annotation speed and scalability, their performance is highly task-dependent, with accuracy rivaling human annotators on some well-defined tasks but exhibiting critical inconsistencies on others requiring nuanced semantic understanding. This analysis provides a framework for evaluating automated annotation pipelines and discusses their implications for accelerating machine learning development cycles.