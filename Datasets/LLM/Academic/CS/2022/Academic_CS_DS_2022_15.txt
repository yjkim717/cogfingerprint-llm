**Abstract**

The deployment of large-scale Transformer architectures in latency-sensitive environments remains a significant challenge due to their substantial computational and memory footprints. While post-training quantization has been extensively explored, structured pruning—the removal of entire network components—offers a complementary pathway for accelerating inference. This paper introduces a novel, efficient algorithm for inducing structured sparsity in pre-trained Transformer models immediately prior to deployment, eliminating the need for resource-intensive retraining or fine-tuning. Our method leverages an information-theoretic criterion based on the Fisher Information Matrix to identify and prune attention heads and feed-forward network layers with minimal impact on the model's predictive distribution. We demonstrate that our approach induces a form of hardware-friendly, block-level sparsity that translates directly into reduced inference latency on standard accelerator hardware. Empirical evaluations across a range of natural language processing benchmarks show that our method can reduce model size by up to 40% and achieve inference speedups of 1.5x-2.0x on contemporary GPUs, while preserving over 99% of the original model's task accuracy. This work establishes a practical and effective paradigm for compressing and accelerating Transformers for real-world applications.