In self-supervised representation learning, non-contrastive Siamese networks such as SimSiam have demonstrated competitive performance without requiring negative pairs. However, Efros (2022) identifies a critical vulnerability: these architectures are prone to partial dimensional collapse, wherein representations occupy only a subspace of the available embedding dimensions, thereby limiting expressivity. This phenomenon is shown to be highly sensitive to dataset scale and model capacity. To quantify this issue, the authors propose a novel collapse metric that captures the effective dimensionality of learned embeddings. In response, they introduce architectural refinements and a continual learning framework that progressively adapts representations to prevent collapse. Empirical evaluations across multiple vision benchmarks confirm that their approach not only mitigates dimensional collapse but also elevates downstream task accuracy by as much as 18 percentage points compared to standard implementations. These findings underscore the necessity of scalable, collapse-resistant designs for robust self-supervised learning and highlight the interplay between dataset characteristics, model size, and representation quality.