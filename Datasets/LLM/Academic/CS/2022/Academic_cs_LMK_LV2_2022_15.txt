Title: Optimizing Task Prompts for Pre-trained Language Models via Black-Box Tuning

Abstract:
The increasing prevalence of Language-Model-as-a-Service (LMaaS) has led to a paradigm shift in natural language processing, where large pre-trained language models are employed as black-box APIs. However, optimizing task-specific prompts for these models remains a significant challenge. In this paper, we propose a novel black-box tuning framework that leverages derivative-free optimization to fine-tune task prompts for large pre-trained language models. By treating the LMaaS as a black-box function, our approach eliminates the need for gradient information and model internals, enabling seamless integration with off-the-shelf LMaaS platforms. We demonstrate the efficacy of our framework through extensive experiments on various NLP tasks, showcasing its ability to optimize task prompts and achieve competitive performance with state-of-the-art prompt tuning methods. Our results highlight the potential of black-box tuning as a versatile and efficient solution for adapting pre-trained language models to downstream tasks in the LMaaS era.