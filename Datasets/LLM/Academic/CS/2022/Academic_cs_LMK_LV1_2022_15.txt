Title: Optimizing Task Prompts for Pre-Trained Language Models via Black-Box Tuning

Abstract:
The increasing popularity of Language-Model-as-a-Service (LMaaS) has led to widespread adoption of pre-trained language models (PLMs) in various natural language processing tasks. However, optimizing task-specific prompts for these models remains a challenging task, particularly when the model's internal workings are inaccessible. To address this limitation, we propose a novel black-box tuning framework that leverages derivative-free optimization techniques to optimize task prompts for large PLMs. By treating the PLM as a black box, our framework avoids the need for gradient information, enabling efficient prompt tuning without requiring access to the model's internal parameters. We demonstrate the effectiveness of our approach through extensive experiments on a range of NLP tasks, showcasing its potential to improve the performance of PLMs in real-world applications. Our work contributes to the development of more efficient and adaptable LMaaS systems, facilitating the widespread adoption of PLMs in 2022 and beyond.