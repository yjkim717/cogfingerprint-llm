This paper introduces a graph-attentive deep reinforcement learning framework for dynamic job shop scheduling optimization. By modeling scheduling environments as disjunctive graphs, our method employs a message-passing neural architecture to capture complex inter-dependencies among operations and machines. The learned policy network iteratively selects and applies improvement heuristics through a Markov decision process formulation, enabling adaptive search in large solution spaces. Experimental evaluation across classical benchmark instances demonstrates that our approach achieves significant improvements over both conventional dispatching rules and recent learning-based methods. The proposed framework advances automated scheduling systems by combining the representational power of graph neural networks with the decision-making capability of deep reinforcement learning, providing new insights into combinatorial optimization under uncertainty.