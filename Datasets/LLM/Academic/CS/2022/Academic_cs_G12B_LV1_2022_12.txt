
## ConvMixer: Re-evaluating the Role of Attention in Patch-Based Image Processing

**Abstract**

The recent surge in popularity of Vision Transformers (ViTs) has prompted a re-examination of the fundamental building blocks of image processing architectures. While ViTs leverage self-attention mechanisms to capture global dependencies, their computational cost and sensitivity to training data volume have presented ongoing challenges. This work investigates the potential of convolutional operations in patch-based image processing, presenting ConvMixer, a novel architecture that eschews self-attention entirely in favor of a deeply stacked sequence of spatially separated depthwise convolutional layers interleaved with pointwise convolutional layers. We demonstrate that by carefully designing the patch embedding and convolutional structure, ConvMixer can achieve competitive, and in some cases superior, performance to ViTs and other state-of-the-art convolutional neural networks (CNNs) on standard image classification benchmarks such as ImageNet. 

Our analysis suggests that the spatial mixing capabilities provided by depthwise convolutions, coupled with the channel mixing facilitated by pointwise convolutions, effectively capture long-range dependencies within image patches, rivaling the functionality of self-attention.  Critically, ConvMixer exhibits significantly improved training efficiency and reduced computational complexity compared to ViTs, particularly when scaling to larger model sizes. We conduct extensive ablation studies to analyze the impact of different architectural choices, highlighting the importance of both spatial and channel mixing operations. These results challenge the prevailing assumption that self-attention is indispensable for achieving high performance in patch-based vision models, offering a compelling alternative that may prove more practical for resource-constrained environments and applications requiring rapid training.



---
