This paper investigates the implicit regularization properties of gradient-based optimization in neural networks learning structured functions over binary inputs. We establish a theoretical connection between the trajectory of gradient descent and the spectral characteristics of the learned function, demonstrating that optimization dynamics naturally favor representations with concentrated Fourier spectra. Through rigorous analysis of the training dynamics on Boolean function classes, we prove that gradient descent exhibits a spectral bias toward low-degree basis functions, with convergence rates dependent on the target function's influence distribution. Our framework provides novel generalization bounds expressed in terms of the total influence and noise sensitivity of the learned hypothesis, revealing how optimization implicitly controls complexity through spectral concentration. Empirical studies validate that networks trained via gradient descent achieve superior generalization on functions with sparse Fourier representations, while struggling with high-frequency components. These findings offer new insights into the algorithmic regularization mechanisms underlying neural network generalization, with implications for architecture design and theoretical understanding of deep learning.