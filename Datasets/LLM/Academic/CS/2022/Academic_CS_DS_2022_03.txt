**Abstract**

This work establishes new computational barriers for learning linear classifiers in the presence of structured, adversarial label noise. We investigate the Probably Approximately Correct (PAC) learnability of halfspaces under a refined noise model that interpolates between the random classification noise and the fully adversarial scenario. Our central result demonstrates that, assuming the intractability of the Learning With Errors (LWE) problem, no polynomial-time algorithm can PAC learn homogeneous halfspaces while achieving a misclassification rate that surpasses a fundamental threshold proportional to the noise rate. This lower bound holds even when the noise corrupts only a minority of the labels and is bounded away from the information-theoretic limit of 1/2. This finding provides a compelling explanation for the observed performance gaps of efficient learning algorithms in this setting, positioning the problem's computational complexity squarely within the realm of lattice-based cryptography. Consequently, our results imply that achieving statistically optimal error rates for this fundamental learning task is computationally infeasible, barring a major breakthrough in complexity theory.