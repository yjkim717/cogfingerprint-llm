Title: Mitigating Dimensional Collapse in Non-Contrastive Siamese Representation Learning for Continual Knowledge Acquisition

Abstract:
Non-contrastive Siamese representation learning methods have garnered significant attention in the realm of self-supervised learning, owing to their ability to learn robust representations without relying on labeled data. However, our analysis reveals that these methods are susceptible to dimensional collapse, particularly when confronted with varying dataset sizes and model capacities. In this study, we introduce a novel metric to quantify dimensional collapse, thereby facilitating a more nuanced understanding of its impact on representation learning. Our findings indicate that dimensional collapse is exacerbated by increased model capacity and reduced dataset sizes. Building upon these insights, we propose a regularization technique to mitigate dimensional collapse, demonstrating its efficacy in promoting continual knowledge acquisition. Our results underscore the importance of addressing dimensional collapse in non-contrastive Siamese representation learning, with implications for the development of more robust and adaptable self-supervised learning frameworks.