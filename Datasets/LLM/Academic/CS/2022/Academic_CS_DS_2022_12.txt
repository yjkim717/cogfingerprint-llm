**Abstract**  
This paper introduces a computational framework for robust albedo estimation from multi-view aerial imagery, addressing key challenges in large-scale photogrammetric reconstruction. Traditional methods often conflate intrinsic scene properties with extrinsic illumination effects, leading to inaccuracies in feature correspondence and dense matching. Our approach integrates a physics-based intrinsic image decomposition model with a hierarchical feature matching pipeline to decouple albedo from shading and shadowing artifacts. By leveraging learned descriptors and iterative refinement, the system enhances the discriminative power of feature representations across varying illumination conditions. We validate our method on urban and natural terrain datasets, demonstrating significant improvements in both alignment accuracy and 3D reconstruction fidelity compared to state-of-the-art techniques. The proposed solution offers a scalable, illumination-invariant foundation for applications in environmental monitoring, autonomous navigation, and digital twin generation.  

*Keywords: albedo estimation, multi-view stereo, illumination invariance, feature representation, 3D reconstruction.*