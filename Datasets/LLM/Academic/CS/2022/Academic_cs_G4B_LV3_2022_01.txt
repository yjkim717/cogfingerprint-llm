Here’s an academic abstract based on the provided summary and keywords, suitable for a 2022 publication:

**Abstract**

Recent advancements in self-supervised learning (SSL) utilizing Siamese representation architectures, exemplified by SimSiam, have demonstrated a tendency toward partial dimensional collapse – a phenomenon where the learned representations disproportionately focus on a subset of features. This study examines this instability within non-contrastive SSL frameworks, hypothesizing a direct correlation between dimensional collapse and subsequent performance degradation in downstream tasks. We introduce a novel metric, termed ‘Representation Stability Score,’ to quantify the extent of collapse and predict its impact on task accuracy.  Experiments utilizing continual learning strategies reveal a significant mitigation of collapse and a corresponding boost in generalization ability.  Specifically, incorporating task-specific regularization during continual training effectively prevents the selective feature prioritization observed in collapsed representations.  These findings underscore the importance of addressing dimensional collapse for robust and reliable SSL models, particularly within the context of continual learning scenarios.