In recent years, the paradigm of Language-Model-as-a-Service (LMaaS) has gained prominence, yet it restricts gradient access, rendering conventional fine-tuning infeasible. This paper introduces black-box tuning, a novel approach that optimizes continuous prompts via derivative-free optimization (DFO) within low-dimensional subspaces. By leveraging the low intrinsic dimensionality of loss landscapes, our method circumvents the need for gradient computations while maintaining high performance. Empirical evaluations demonstrate that black-box tuning significantly outperforms both manual prompt engineering and gradient-based techniques in few-shot learning scenarios across diverse benchmarks. These findings underscore the efficacy of DFO in harnessing the representational power of large language models under black-box constraints, offering a scalable and practical alternative for resource-limited environments. This work advances prompt-based adaptation strategies and highlights the untapped potential of zeroth-order optimization in natural language processing.