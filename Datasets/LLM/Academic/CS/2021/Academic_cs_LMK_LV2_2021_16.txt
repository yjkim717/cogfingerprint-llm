This study evaluates the efficacy of IndicBART, a pre-trained sequence-to-sequence model tailored for Indic languages, in natural language generation (NLG) tasks. Leveraging multilingual training and transfer learning paradigms, IndicBART demonstrates competitive performance with larger models despite its relatively compact size. Our analysis reveals that the model's proficiency in capturing linguistic nuances and generating coherent text is attributable to its pre-training on a diverse dataset comprising multiple Indic languages. The findings suggest that IndicBART offers a viable solution for NLG tasks in Indic languages, underscoring the potential of targeted pre-training and transfer learning in enhancing model efficiency and effectiveness.