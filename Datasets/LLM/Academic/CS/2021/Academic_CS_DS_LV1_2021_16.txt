**Abstract**

The proliferation of multilingual sequence-to-sequence models has primarily favored high-resource languages, creating a performance gap for linguistically diverse, low-resource language families. This paper introduces IndicBART, a compact, pre-trained encoder-decoder model specifically designed for 11 Indic languages. By leveraging a shared subword vocabulary and a curated multilingual corpus, our model is trained using a denoising autoencoder objective. Despite its significantly smaller parameter count compared to contemporary multilingual models like mBART, IndicBART demonstrates competitive performance on key natural language generation tasks, including neural machine translation and abstractive summarization. Our findings challenge the prevailing assumption that model scale is the primary determinant of performance, suggesting instead that targeted, linguistically coherent pre-training can yield highly efficient and effective models for specific language families. This work provides a scalable framework for building performant NLP systems for other underrepresented language groups.