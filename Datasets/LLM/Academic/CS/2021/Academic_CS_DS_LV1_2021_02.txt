**Abstract**

We establish the statistical optimality of polynomial regression for distribution-specific agnostic learning under isotropic Gaussian covariates. Our central result demonstrates that for any concept class, the polynomial regression estimator of degree *k* achieves the minimax optimal agnostic error rate, provided the target function resides in the *Lâ‚‚*-closure of the class. This finding positions polynomial regression as a canonical and computationally efficient algorithm for this fundamental learning setting. Complementing this, we derive strong Statistical Query (SQ) lower bounds that characterize the intrinsic difficulty of agnostic learning for specific function classes. We show that learning linear threshold functions (LTFs) and low-degree polynomial threshold functions (PTFs) over the Gaussian space requires super-polynomial complexity or queries of exponentially small tolerance within the SQ model. These lower bounds delineate a sharp computational separation, underscoring that while polynomial regression is statistically optimal, efficiently achieving this optimality for certain classes is information-theoretically precluded in the SQ framework, thereby refining our understanding of the computational-statistical trade-offs in agnostic learning.

*(Word Count: 153)*