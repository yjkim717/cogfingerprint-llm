This paper investigates the computational complexity of distribution-specific agnostic learning through the lens of statistical query (SQ) lower bounds. We establish a fundamental separation between learning with respect to Gaussian distributions versus general distributions by demonstrating that certain concept classes admit efficient learning algorithms under Gaussian marginals while remaining provably hard in the general agnostic setting. Our central technical contribution develops a novel duality framework connecting approximation theory in Lp spaces to information-theoretic SQ lower bounds. Using this machinery, we prove that for broad families of polynomial-based concept classes, no SQ algorithm can achieve significantly better error rates than polynomial regression when the underlying distribution is standard Gaussian. These results provide a precise characterization of the limitations of local search methods for distribution-specific learning and highlight the surprising optimality of classical polynomial regression in this structured setting. Our findings have implications for the complexity of robust machine learning and suggest inherent barriers for efficiently learning certain geometric concepts even under well-behaved distributions.