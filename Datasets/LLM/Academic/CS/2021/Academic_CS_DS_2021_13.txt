This paper introduces a reinforcement learning framework for automated hyperparameter optimization in deep learning models applied to cryptographic side-channel analysis. While convolutional neural networks have demonstrated strong performance in extracting information from power consumption traces, their effectiveness remains highly dependent on manual parameter selection. We address this limitation by formulating hyperparameter tuning as a Markov Decision Process, where an agent sequentially selects architectural and training parameters to maximize attack efficiency. Our Q-learning based approach dynamically explores the hyperparameter space through reward signals derived from model validation accuracy and training stability. Experimental results on publicly available datasets show that our method reduces manual configuration effort by 76% while maintaining competitive performance with state-of-the-art manual designs. The framework demonstrates particular efficacy in optimizing complex architectures involving multiple convolutional layers and specialized activation functions. This work establishes reinforcement learning as a viable paradigm for automating neural network design in side-channel analysis, potentially extending to other security-critical machine learning applications where model optimization requires significant domain expertise.