Title: Investigating Cross-Situational Word Learning in Multimodal Neural Networks

Abstract:
Cross-situational word learning, a fundamental aspect of human language acquisition, involves inferring word-referent mappings from ambiguous contexts. Recent studies have explored the capacity of multimodal neural networks to model this phenomenon. In this work, we examine the extent to which these models can replicate behavioral patterns observed in human cross-situational word learning, particularly with respect to the principles of mutual exclusivity and visual-linguistic mappings. We train a multimodal neural network on a dataset comprising paired visual and linguistic stimuli, and evaluate its performance on word-referent mapping tasks. Our results demonstrate that the model successfully learns to disambiguate word meanings across situations, leveraging both visual and linguistic cues. Furthermore, we show that the model's behavior is consistent with the mutual exclusivity principle, a key constraint guiding human word learning. Our findings suggest that multimodal neural networks offer a viable framework for understanding the cognitive mechanisms underlying cross-situational word learning, and highlight the potential for future research at the intersection of language acquisition and multimodal processing. (2021)