 Here's a formal academic-style abstract, inspired by the provided summary and keywords, suitable for a CS publication (circa 2021):

This work analyzes agnostic learning within the framework of Gaussian noise models, specifically focusing on the efficacy of L1-regularized regression. We rigorously establish the optimality of L1-regression for Boolean-valued function classes under these conditions. Furthermore, we derive sharp or near-optimal Sample-Query (SQ) lower bounds for a diverse range of function classes, including linear threshold functions, polynomial threshold functions, and compositions of common activation functions like ReLUs and sigmoids. Our results leverage techniques from LP duality to provide novel insights into the fundamental limits of agnostic learning, offering a refined understanding of query complexity and generalization performance.