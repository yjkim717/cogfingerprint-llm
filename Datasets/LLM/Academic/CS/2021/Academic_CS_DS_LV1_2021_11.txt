In this work, we address the challenge of weakly supervised segmentation, where neural networks must learn from incomplete or noisy labels. We propose a novel optimization framework that integrates a robust trust region method to systematically incorporate geometric priors through regularized losses. By constraining parameter updates within a trust region, our approach enables stable collaboration between deep networks and strong low-level solvers—such as graph cuts or convex optimization methods—that are sensitive to initialization. This synergy mitigates error propagation from weak supervision while preserving structural constraints. Experiments on benchmark datasets demonstrate that our method achieves significant improvements in segmentation accuracy and robustness over adversarial examples, outperforming contemporary alternatives that rely solely on heuristic loss functions or post-processing. Our findings highlight the value of embedding optimization-theoretic principles into weakly supervised learning pipelines.