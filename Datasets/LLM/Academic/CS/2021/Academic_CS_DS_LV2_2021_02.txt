This paper establishes fundamental statistical and computational limits for agnostic learning under Gaussian distributions. We demonstrate that polynomial regression achieves information-theoretic optimality for this learning model, providing a tight characterization of its sample complexity. Complementing this positive result, we derive strong Statistical Query (SQ) lower bounds for learning broad concept classes within this framework. Specifically, we prove that learning linear threshold functions under Gaussian distributions requires super-polynomial SQ complexity in the agnostic setting. These lower bounds extend to other natural concept classes and reveal inherent limitations of efficient agnostic learning algorithms. Our results provide a unifying perspective on the computational-statistical tradeoffs in distribution-specific learning, showing that while polynomial regression is optimal from a statistical perspective, computationally efficient agnostic learning remains provably hard for fundamental concept classes under standard complexity-theoretic assumptions.