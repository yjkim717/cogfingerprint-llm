Here’s an academic-style abstract based on the provided summary, suitable for a 2021 publication:

**Abstract**

The burgeoning field of natural language processing (NLP) increasingly demands robust models capable of handling diverse linguistic landscapes. This paper introduces IndicBART, a novel pre-trained sequence-to-sequence model specifically tailored for eleven Indic languages alongside English. Leveraging transfer learning techniques, IndicBART capitalizes on a foundational pre-training phase to establish a strong representation space, subsequently demonstrating competitive efficacy across Neural Machine Translation (NMT) and text summarization tasks.  Our methodology centers on adapting a BART architecture, a proven transformer-based model, to the unique characteristics of Indic scripts and grammatical structures.  Evaluations reveal that IndicBART significantly outperforms baseline models trained from scratch on comparable datasets, highlighting the benefits of pre-training for low-resource languages.  Further research will investigate incorporating multilingual training strategies and exploring the model’s adaptability to other downstream NLP applications within the Indic language family.