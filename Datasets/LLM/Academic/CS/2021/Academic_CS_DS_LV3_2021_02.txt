In this 2021 theoretical computer science study, we establish fundamental limits for agnostic learning under Gaussian distributions by leveraging LP duality and statistical query (SQ) frameworks. Our primary contribution demonstrates that polynomial regression achieves information-theoretic optimality for Boolean concept classes, with LÂ¹-regression emerging as the computationally efficient minimax strategy. Through novel SQ lower bound constructions, we prove that neither linear nor polynomial threshold functions admit efficient agnostic learning under Gaussian marginals, resolving open questions about the complexity of these fundamental hypothesis classes. The analysis reveals an intrinsic alignment between polynomial regression and the Gaussian distribution's moment structure, providing a unifying perspective on statistical-computational trade-offs in high-dimensional learning. These results delineate precise boundaries for efficient agnostic learning while introducing new techniques for deriving information-theoretic lower bounds through duality arguments.