**Abstract**

Cross-situational word learning (CSWL), the capacity to acquire novel word meanings based on observed interactions between objects and actions, presents a significant challenge for computational models of language acquisition. This research explores the potential of multimodal neural networks to replicate key aspects of CSWL, specifically focusing on the emergent formation of visual-linguistic and word-referent mappings. We hypothesize that architectures incorporating both visual and linguistic data streams can, through unsupervised learning, develop representations that capture the statistical relationships underlying word meanings within novel contexts. 

Our investigation utilizes a framework predicated on mutual exclusivity – the principle that a word’s meaning is constrained by the specific referents encountered during learning – to evaluate network performance.  Employing a simulated CSWL environment, we assess the ability of a multimodal neural network to learn and retain consistent word-referent associations. Preliminary results indicate that networks trained with appropriate architectural constraints exhibit improved fidelity to observed CSWL patterns, suggesting a viable pathway for modeling this fundamental aspect of human language development within a machine learning paradigm.