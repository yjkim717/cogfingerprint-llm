)
## Orthogonal Convolutional Layers via Cayley Parameterization for Enhanced Adversarial Robustness

The increasing vulnerability of deep neural networks to adversarial attacks necessitates the development of robust architectures. While various defense strategies exist, many introduce significant computational overhead or compromise accuracy on clean data. This work addresses the challenge of adversarial robustness in convolutional neural networks (CNNs) by proposing a novel parameterization of convolutional layers based on the Cayley transform. Specifically, we directly enforce orthogonality within the convolutional filter weights, a property known to contribute to improved robustness. Traditional methods of achieving orthogonal convolutions often rely on iterative optimization or specialized layer designs, hindering scalability. Our approach leverages the Cayley transform to map a standard parameter space to one where orthogonality is intrinsically maintained, offering a computationally efficient and stable alternative. We demonstrate that this Cayley parameterization allows for the construction of deep CNNs with orthogonal convolutional layers, facilitating robust training without requiring complex regularization techniques. Furthermore, we explore the connection between orthogonality and Lipschitz constants, suggesting that the induced parameterization implicitly controls network behavior and contributes to improved generalization. Experimental results on benchmark datasets (CIFAR-10, ImageNet) demonstrate enhanced adversarial robustness against common attacks while maintaining competitive clean accuracy, and scaling effectively to larger network architectures.



