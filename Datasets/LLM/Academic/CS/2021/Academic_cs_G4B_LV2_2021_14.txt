**Abstract**

The increasing prevalence of autonomous vehicles necessitates robust and reliable collision avoidance systems. This research investigates the efficacy of reinforcement learning (RL) algorithms, specifically Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC), in achieving multi-vehicle collision avoidance within a simulated environment. Employing a centralized control architecture, we evaluated the performance of both PPO and SAC, leveraging their distinct exploration strategies to optimize vehicle trajectories and minimize potential collisions.  The study hypothesizes that SAC’s entropy regularization will provide a more stable and adaptable policy, particularly in scenarios demanding nuanced decision-making. 

Simulation experiments utilized a configurable traffic network to generate diverse collision scenarios, allowing for a comprehensive assessment of each algorithm’s collision avoidance capabilities. Performance metrics included collision frequency, average time to collision, and overall system efficiency. Preliminary results indicate that while PPO demonstrated competitive performance, SAC consistently outperformed in terms of collision reduction and stability, likely attributable to its enhanced exploration and robustness.  Further analysis will explore the impact of hyperparameter tuning and network complexity on the algorithms’ effectiveness.  These findings contribute to the growing body of research exploring RL’s potential for safe and efficient autonomous vehicle control, with implications for future system design and validation.