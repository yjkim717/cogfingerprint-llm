In this 2021 computational study, we conduct a systematic comparison of two prominent deep reinforcement learning algorithms—Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC)—for centralized multi-vehicle collision avoidance in autonomous driving systems. Our methodology employs high-fidelity Unity3D simulations to model complex urban traffic scenarios, where a centralized controller coordinates vehicle trajectories. The experimental framework evaluates algorithmic performance through three critical traffic flow metrics: average speed differentials, positional deviation variances, and lane-change frequency. Results demonstrate that both PPO and SAC achieve collision avoidance success rates of 91% in dense traffic conditions, though with distinct behavioral characteristics. PPO exhibits more stable policy convergence during training, while SAC shows superior sample efficiency in continuous action spaces. Quantitative analysis reveals SAC maintains 15% lower speed variance during avoidance maneuvers, whereas PPO achieves 12% fewer unnecessary lane changes. These findings suggest algorithm selection represents a trade-off between motion smoothness and decisional conservatism in multi-agent navigation. This research provides empirical benchmarks for reinforcement learning applications in connected autonomous vehicle networks and establishes evaluation protocols for emergent behaviors in cooperative driving systems.