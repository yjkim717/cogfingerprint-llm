**Abstract**

Contemporary algorithmic decision-making systems increasingly permeate critical societal domains, necessitating rigorous examination of their potential for perpetuating and amplifying existing inequalities. This paper critically assesses prevailing conceptions of algorithmic fairness, contending that they frequently operate within narrow, statistical frameworks, neglecting the fundamental role of power dynamics and systemic biases. We posit that a purely metric-based approach to auditing – focusing solely on disparate impact – fails to capture the causal pathways through which algorithmic outcomes contribute to broader societal disparities. 

Leveraging insights from causal inference, we advocate for a shift towards a more holistic evaluation methodology. This necessitates incorporating methods capable of identifying and quantifying the influence of structural factors and historical injustices on algorithmic performance. Specifically, we propose an audit framework centered on examining the causal relationships between input variables, algorithmic processes, and observed outcomes, alongside an assessment of the power structures embedded within the system’s design and deployment.  Such an approach, developed in 2021, offers a more nuanced understanding of fairness beyond simple statistical parity.