In weakly supervised segmentation, the absence of dense annotations necessitates reliance on regularization to guide neural network optimization. However, integrating strong geometric priors or low-level regularizers often destabilizes training due to conflicting gradient signals. This paper introduces a robust trust region optimization framework that mitigates this issue by constraining parameter updates to regions where both the data fidelity and regularizer remain trustworthy. Our method enables the effective incorporation of powerful regularized losses—such as boundary length or spatial smoothness terms—without compromising convergence. Experiments on biomedical and natural image datasets demonstrate that our approach achieves significant improvements in segmentation accuracy and stability over baseline methods, effectively bridging the gap between weakly supervised and fully supervised performance. The proposed technique offers a principled pathway to leverage domain knowledge in data-scarce scenarios.