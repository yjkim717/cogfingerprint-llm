**Abstract**

Weakly supervised semantic segmentation presents a significant challenge due to the scarcity of ground-truth annotations. This work addresses this limitation by proposing a novel trust region optimization framework designed to effectively incorporate regularized losses within a deep learning architecture. Our approach strategically integrates strong, low-level solvers to mitigate the ambiguities inherent in weak supervision, thereby improving segmentation accuracy. Specifically, we employ a trust region method to tune the modelâ€™s parameters, ensuring stable convergence and robust performance. Crucially, we incorporate geometric priors into the loss function, encouraging spatially consistent segmentations and reducing reliance on potentially noisy weak labels. Experimental results on benchmark datasets demonstrate that our method surpasses existing weakly supervised techniques, achieving state-of-the-art results while requiring substantially fewer annotations. The findings highlight the potential of regularized losses and trust region optimization for advancing weakly supervised segmentation in 2021 and beyond.