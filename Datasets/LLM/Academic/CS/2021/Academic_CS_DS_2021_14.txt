This paper introduces a novel framework for constructing provably Lipschitz-constrained deep neural networks through structured orthogonal parameterizations of convolutional operators. While existing certified defense methods against norm-bounded adversarial attacks often rely on expensive post-hoc verification or compromise architectural expressiveness, our approach embeds orthogonality constraints directly into network design via efficient matrix exponential mappings. We demonstrate that maintaining strict orthogonality in convolutional filters yields tighter Lipschitz bounds throughout the network, enabling more accurate certified robustness guarantees against ℓ₂ perturbations without sacrificing training stability. Our method achieves state-of-the-art certified accuracy on CIFAR-10 under varying threat models while maintaining computational efficiency comparable to standard architectures. The proposed orthogonal convolutions additionally exhibit improved gradient propagation and feature preservation properties, suggesting broader applications beyond adversarial defense. Empirical evaluations confirm that our method provides substantial improvements over existing certified defense approaches while maintaining competitive clean accuracy, establishing a new direction for developing theoretically grounded robust architectures.