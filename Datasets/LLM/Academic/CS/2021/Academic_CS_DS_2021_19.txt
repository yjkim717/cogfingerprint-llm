**Abstract**

The paradigm of prompt-based learning has emerged as a transformative methodology for adapting pre-trained language models (PLMs) to downstream tasks without extensive parameter fine-tuning. This approach re-conceptualizes task-specific objectives as cloze-style or text-infilling problems, enabling models to leverage their pre-existing knowledge more directly. In 2021, research has increasingly focused on systematizing the design of effective prompts, moving beyond manual templates to automated and generative methods. Key challenges include mitigating sensitivity to prompt phrasing, scaling to complex tasks beyond classification, and developing theoretical frameworks to explain the efficacy of few-shot and zero-shot inference. This paradigm shift underscores a broader trend towards unifying diverse NLP problems under a single, text-to-text modeling framework, promising greater model generalization and data efficiency. Future directions involve integrating prompt-based strategies with retrieval-augmented generation and exploring their synergies with emergent reasoning capabilities in large-scale models.