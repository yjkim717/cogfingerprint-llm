This paper introduces IndicBART, a novel multilingual sequence-to-sequence model pre-trained on 11 Indic languages and English. Addressing the persistent challenge of data scarcity in low-resource languages, our approach strategically exploits the high orthographic similarity and shared scripts across the Indic linguistic family. We hypothesize that this linguistic commonality enables more effective cross-lingual transfer learning within a compact model architecture. Our empirical evaluation demonstrates that IndicBART achieves state-of-the-art or competitive performance on benchmarks for neural machine translation and extreme summarization, even with minimal fine-tuning data. These results substantiate that leveraging intrinsic language family characteristics is a viable and efficient alternative to simply scaling model parameters, offering a sustainable pathway for building performant NLP systems for linguistically diverse regions.