Abstract

This 2021 computational study investigates how multimodal neural networks acquire visual-linguistic mappings through cross-situational learning—a process where statistical regularities across multiple exposures enable word-referent associations. We demonstrate that these models successfully replicate key behavioral phenomena observed in human language acquisition, including one-trial learning and cross-situational statistical inference. However, our analysis reveals a critical limitation: neural architectures exhibit weaker inductive biases for mutual exclusivity—the cognitive constraint favoring one-to-one mappings between words and referents—compared to human learners. Through controlled simulations with varying noise levels and referent distributions, we establish that while networks statistically converge toward correct mappings, they frequently violate mutual exclusivity principles by accepting multiple labels for single objects. This divergence suggests that human word learning relies on stronger prior structural constraints than those emergent in current neural systems. Our findings highlight fundamental differences between data-driven and cognitively-guided learning, proposing that integrating explicit inductive biases could bridge this gap. This research contributes to more cognitively plausible computational models of language acquisition while offering insights for developmental psychology through mechanistic interpretability of artificial learning systems.

Word count: 219