**Abstract**

The dominant paradigm in algorithmic fairness research has prioritized statistical parity metrics derived from observational data, an approach that fails to address the underlying structural inequities algorithmic systems often perpetuate. This paper posits that a rigorous, causal-inference-based framework is necessary to move beyond correlative fairness and toward a substantive analysis of algorithmic power distribution. We argue that by modeling the causal pathways through which an algorithm allocates resources and opportunities, we can formally audit its role in either mitigating or exacerbating existing social hierarchies. Our proposed methodology integrates counterfactual analysis with a power-aware taxonomy of harms, shifting the evaluative focus from group-based error rates to the mechanisms of distributive inequality. This reconceptualization provides a more robust foundation for algorithmic auditing, demanding accountability not merely for biased predictions, but for the reification of power asymmetries. The year 2021 marks a critical juncture for adopting such structural analyses as computational systems become further embedded in consequential social institutions.