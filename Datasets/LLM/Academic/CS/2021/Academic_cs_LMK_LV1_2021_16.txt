Recent advancements in natural language generation (NLG) have been driven by large pre-trained sequence-to-sequence models. However, these models are often computationally expensive and linguistically restricted. In this work, we examine the efficacy of IndicBART, a pre-trained multilingual model tailored to Indic languages, in NLG tasks. Despite its relatively compact size, IndicBART demonstrates competitive performance in generating coherent and contextually relevant text. We evaluate IndicBART on a range of NLG tasks, including machine translation and text summarization, and observe that it achieves comparable results to larger pre-trained models. Our findings suggest that IndicBART's linguistically informed architecture and pre-training objectives enable it to effectively capture the nuances of Indic languages. This study highlights the potential of specialized multilingual models like IndicBART for NLG tasks, particularly in low-resource language settings, and underscores the importance of linguistic diversity in NLG research (as of 2021).