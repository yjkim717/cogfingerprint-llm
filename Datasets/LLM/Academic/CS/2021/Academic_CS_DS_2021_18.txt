This paper presents a comparative analysis of proximal policy optimization (PPO) and soft actor-critic (SAC) reinforcement learning algorithms for decentralized collision avoidance in autonomous vehicle navigation systems. We formulate the multi-agent interaction problem as a partially observable Markov decision process where each agent receives only local sensor observations. Our methodology implements both on-policy (PPO) and off-policy (SAC) learning paradigms within identical simulation environments featuring complex urban scenarios with dynamic obstacles and unpredictable pedestrian movements. Experimental results demonstrate that PPO achieves superior sample efficiency during training, converging to effective policies 40% faster than SAC counterparts. However, SAC agents ultimately attain slightly higher asymptotic performance in dense traffic conditions, successfully navigating 94.2% of challenging scenarios versus 92.8% for PPO. Both approaches significantly outperform traditional rule-based systems, which achieved only 76.3% success rates under identical test conditions. The study provides insights into the trade-offs between policy optimization methods for safety-critical applications and establishes benchmarks for future research in multi-agent reinforcement learning for autonomous systems.