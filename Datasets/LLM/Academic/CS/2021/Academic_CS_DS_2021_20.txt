Abstract: Recent advances in parameter-efficient adaptation have demonstrated the viability of prompt-based methods for few-shot learning across diverse NLP tasks. However, these approaches face significant challenges in lifelong learning scenarios where models must continuously acquire new capabilities while preserving previously learned knowledge. This paper introduces a novel framework that integrates dynamic prompt composition with synthetic task exemplars to address catastrophic forgetting in resource-constrained environments. Our method employs a dual-component architecture where (1) task-specific prompts are generated through meta-learned transformations of a shared prompt bank, and (2) pseudo-samples are synthesized using calibrated language generation to approximate historical task distributions. We formulate the learning objective using information-theoretic regularization, incorporating KL divergence constraints to maintain stability across sequential task acquisitions. Extensive evaluations on benchmark datasets demonstrate that our approach achieves state-of-the-art performance in continual few-shot learning settings, outperforming conventional fine-tuning methods by 7.3% in average accuracy while requiring 0.5% of additional parameters per task. These results establish new possibilities for developing sustainable NLP systems that can evolve through sequential task learning without expensive retraining procedures.