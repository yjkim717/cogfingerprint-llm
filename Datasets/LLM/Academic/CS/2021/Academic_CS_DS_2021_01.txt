This paper introduces a novel framework for pose-guided scene synthesis using generative adversarial networks. Building upon StyleGAN2 architecture, our method conditions image generation on precise human pose inputs to produce photorealistic environments that maintain spatial coherence with articulated figures. We propose a pose encoding module that extracts hierarchical features from skeleton keypoints, enabling the generator to interpret subtle kinematic relationships and translate them into contextual scene elements. The model is trained on a large-scale dataset of human-scene interactions, learning to associate body configurations with plausible environmental layouts and object placements. Quantitative evaluation demonstrates significant improvements over existing approaches, with a 23% reduction in Fr√©chet Inception Distance compared to state-of-the-art methods. Our ablation studies confirm the importance of multi-scale pose feature extraction for maintaining spatial consistency. This work advances controllable image generation by demonstrating how structural human priors can guide complex scene formation while preserving visual realism.