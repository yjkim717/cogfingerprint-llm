This paper introduces a novel computational framework for large-scale 3D scene reconstruction from sparse point cloud data. Our approach addresses the fundamental challenge of surface completion in occluded regions through a multi-stage pipeline combining learned visibility analysis with global optimization. We first train a convolutional network to predict virtual viewpoint visibility maps, enabling robust identification of surface boundaries and occluded areas. These predictions then guide a depth completion module that generates dense geometric representations. Finally, we formulate mesh generation as an energy minimization problem solved through graph-cut optimization, ensuring globally consistent surface reconstruction while preserving fine geometric details. Experimental validation on multiple urban-scale datasets demonstrates significant improvements in reconstruction accuracy and computational efficiency compared to existing methods. Our framework achieves state-of-the-art performance in handling complex scenes with varying point density and substantial occlusion, advancing the practical deployment of 3D reconstruction in applications ranging from autonomous navigation to digital preservation.