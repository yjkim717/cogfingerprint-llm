This paper examines the limitations of individual-centric fairness metrics in algorithmic systems, arguing they fail to address structural inequities in power distribution. Drawing on causal inference methods, we develop a framework for algorithmic auditing that models how automated decisions reproduce societal hierarchies. Our analysis reveals that conventional fairness approaches often optimize for parity within existing power structures rather than challenging their underlying inequities. We propose power-aware evaluation metrics that assess algorithms' longitudinal impact on resource allocation across social groups. Through case studies in credit scoring and recidivism prediction, we demonstrate how this approach reveals different forms of algorithmic harm than standard fairness audits. The findings suggest that meaningful algorithmic accountability requires explicit consideration of how technical systems mediate social power, moving beyond narrow statistical parity toward transformative justice.