This paper introduces a novel multilingual sequence-to-sequence architecture optimized for low-resource language families. We present a compact transformer-based model pre-trained simultaneously on twelve South Asian languages using a unified script representation. Our approach combines dense multilingual pretraining with task-specific fine-tuning for neural machine translation and abstractive summarization. Despite utilizing only 40% of the parameters of comparable multilingual models, our system achieves state-of-the-art performance across multiple evaluation benchmarks. The model demonstrates particularly strong cross-lingual transfer capabilities between genealogically related languages, suggesting that language family specialization may be more effective than general-purpose multilingual approaches for certain linguistic domains. These findings have significant implications for developing efficient NLP solutions for linguistically diverse regions with limited computational resources.