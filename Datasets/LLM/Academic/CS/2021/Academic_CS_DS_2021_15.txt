**Abstract**

The escalating computational and memory demands of contemporary deep neural networks present significant deployment challenges, particularly for resource-constrained edge devices and latency-sensitive applications. This paper provides a systematic analysis of post-training and quantization-aware training (QAT) methodologies designed to ameliorate these constraints through the strategic conversion of network parameters and activations from 32-bit floating-point (FP32) representations into low-bitwidth fixed-point integers. We investigate the intricate trade-offs between model compression, computational acceleration, and predictive accuracy, with a specific focus on the non-uniform quantization techniques and mixed-precision strategies that have emerged to mitigate accuracy degradation. The discussion critically assesses the hardware implications of these approaches, highlighting how integer arithmetic logic units (ALUs) common in mobile CPUs and specialized AI accelerators can achieve substantial gains in operations per second (OPS) and energy efficiency. Furthermore, we address the practical challenges of quantizing complex architectures, including attention mechanisms in transformers and lightweight mobile networks, and explore the role of calibration algorithms in determining optimal dynamic ranges. By synthesizing recent advancements up to 2021, this work aims to furnish a clear framework for practitioners navigating the design space of efficient neural network inference, underscoring quantization as an indispensable tool for bridging the gap between model complexity and hardware capability.