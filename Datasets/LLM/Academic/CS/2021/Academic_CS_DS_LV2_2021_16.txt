This paper introduces IndicBART, a compact multilingual sequence-to-sequence model pre-trained on 11 Indic languages. Despite its relatively small parameter size, the model is designed to address the acute resource scarcity for these languages in neural machine translation and abstractive summarization. We demonstrate that through strategic multilingual training on a diverse corpus, IndicBART achieves competitive performance against significantly larger models on multiple benchmarks. Our methodology leverages shared linguistic structures across the Indic language family to enable effective cross-lingual transfer. The results establish that model compression, coupled with targeted multilingual pre-training, is a viable path for building efficient and scalable NLP systems for low-resource languages, offering a practical alternative to monolithic, large-scale architectures.