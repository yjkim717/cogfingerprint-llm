Of course. Here is a formal academic abstract in the field of Computer Science, reflecting the context of 2021.

***

**Abstract**

A fundamental challenge in computational learning is the acquisition of visual-linguistic mappings from ambiguous, cross-situational data, a task human infants solve with remarkable efficiency. While contemporary multimodal neural networks demonstrate proficiency in forming statistical associations between words and visual referents, their performance is often benchmarked against core cognitive constraints, such as the mutual exclusivity biasâ€”the inductive preference to map novel words to novel referents. This paper investigates the limitations of standard neural architectures in exhibiting this form of one-shot disambiguation. We posit that the dominant paradigm of end-to-end training on large-scale, co-occurrence-based datasets encourages distributed, overlapping representations that are inherently non-mutually exclusive. Through a series of controlled simulations, we demonstrate that without explicit architectural priors or structured loss functions, these models fail to suppress previously learned associations when confronted with novel word-referent pairs. Our findings highlight a critical gap between robust statistical learning and flexible, human-like reasoning, suggesting that integrating cognitively-inspired constraints is a necessary direction for developing more efficient and generalizable artificial learning systems.