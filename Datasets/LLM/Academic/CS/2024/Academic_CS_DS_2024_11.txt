**Abstract**  
Recent advances in transformer-based architectures have demonstrated remarkable capabilities in sequence modeling and pattern recognition, yet their performance on structured logical reasoning tasks remains constrained by inherent architectural limitations. This work investigates the relationship between the *globality of attention* and systematic generalization in transformers, particularly in the context of deductive reasoning problems such as syllogistic inference. We propose a novel *inductive scratchpad* methodology, wherein models are trained to generate intermediate reasoning traces that adhere to formal logical rules, rather than relying solely on implicit statistical correlations. Through systematic experiments on synthetic and perturbed real-world benchmarks, we show that this approach significantly enhances out-of-distribution generalization by promoting rule-based reasoning over surface-level pattern matching. Our findings highlight the critical role of explicit intermediate representations in closing the generalization gap for transformer models, offering a pathway toward more robust and interpretable neural reasoning systems.