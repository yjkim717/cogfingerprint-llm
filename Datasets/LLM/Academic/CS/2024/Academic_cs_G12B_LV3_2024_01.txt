 Here's a formal academic-style abstract inspired by the provided summary and keywords, suitable for a CS conference or journal in 2024:

**Abstract:**

Recent advancements in contrastive language-image pre-training (CLIP) models have demonstrated remarkable zero-shot transfer capabilities, yet the underlying mechanisms driving these successes remain incompletely understood. This work introduces a novel analytical framework, termed the "second-order lens," to investigate neuron behavior within CLIP. Departing from traditional activation-based interpretations, we analyze the indirect influence of individual neurons via their interactions across multiple attention heads. Our findings reveal a prevalence of polysemy – where single neurons exhibit diverse and context-dependent representations – significantly exceeding prior assumptions.  Furthermore, exploiting this second-order understanding, we demonstrate the generation of targeted adversarial examples that effectively manipulate CLIP’s zero-shot classification performance.  Finally, we showcase an application of this approach to enhance zero-shot image segmentation, suggesting the potential for refining downstream tasks through a deeper comprehension of CLIP's internal representations. This research contributes to a more nuanced understanding of large-scale vision-language models and opens avenues for improved control and robustness.
