This paper investigates the efficacy of machine learning-driven resource allocation in computational systems, focusing on the trade-offs between predictive accuracy and resultant system-wide inequality. We posit that while predictive models promise optimized resource distribution, their superiority over static, aggregate allocation strategies is contingent upon the initial disparity in resource needs among system components and the total available resource budget. Through simulation and theoretical analysis, we demonstrate that prediction-based methods only yield significant performance gains in environments characterized by low inherent inequality and high resource availability. In contrast, under conditions of high initial disparity or constrained budgets, such methods can exacerbate inequities and underperform simpler, non-predictive approaches. These findings critically delineate the boundaries of data-driven optimization, suggesting that the pursuit of predictive fidelity must be tempered by considerations of systemic fairness and operational constraints.