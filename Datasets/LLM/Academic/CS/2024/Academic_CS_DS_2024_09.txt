In the rapidly evolving landscape of supervised machine learning, robust model evaluation remains a cornerstone of empirical research. This paper surveys contemporary statistical methodologies for comparative analysis of predictive models, addressing the proliferation of complex evaluation metrics across classification, regression, and ranking tasks. We systematically examine hypothesis testing frameworks—including resampling-based approaches and Bayesian alternatives—for determining significant performance differences while controlling for Type I errors. The discussion extends to emerging challenges in deep learning evaluation, where traditional metrics may fail to capture nuanced behavioral differences. Through rigorous benchmarking on diverse datasets, we demonstrate how proper statistical validation prevents overestimation of model capabilities and ensures reproducible conclusions. Our findings emphasize that methodological rigor in performance assessment is equally crucial as architectural innovations in advancing the field.