In multimodal neural networks, polysemantic neurons—individual units responding to multiple unrelated concepts—pose significant interpretability challenges. This paper introduces a second-order attribution framework for decomposing such activations in vision-language models like CLIP. By analyzing how intermediate representations interact across modalities, we demonstrate that seemingly monolithic neuron responses often comprise several distinct semantic features. Our method isolates these components through gradient-based interaction scoring, revealing that polysemantic behavior arises from distributed feature entanglement rather than representational limitations. Furthermore, we establish that these second-order interpretations enable more effective adversarial example generation, where minimal perturbations cause targeted concept suppression while preserving overall model performance. Evaluations show our approach improves zero-shot segmentation robustness by 23% against concept-hybridization attacks while providing mechanistic explanations for previously opaque model decisions. These findings advance fine-grained neural interpretability and demonstrate how higher-order analysis can enhance both understanding and security of multimodal systems.