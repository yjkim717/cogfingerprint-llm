Title: Enhancing Truthfulness in AI Assistants through Self-Assessment of Knowledge Limitations

Abstract:
The proliferation of AI assistants powered by large language models (LLMs) has underscored the imperative for truthfulness in automated question answering. A persistent challenge lies in the propensity of these models to generate "hallucinations" â€“ responses that, while plausible, are factually inaccurate or unsubstantiated. This study addresses this issue by investigating the capacity of AI assistants to recognize and articulate their knowledge limitations. We propose a novel methodology that enables LLMs to self-assess their confidence in responding to queries, thereby improving their truthfulness by judiciously abstaining from answering questions beyond their knowledge scope. Through empirical evaluation, we demonstrate the efficacy of our approach in mitigating hallucinations and enhancing the reliability of AI-driven question answering systems. Our findings have significant implications for the development of more trustworthy AI assistants, a crucial step towards ensuring the safe and beneficial deployment of LLMs in real-world applications.