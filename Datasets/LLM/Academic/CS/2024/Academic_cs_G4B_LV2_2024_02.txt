Here’s an academic abstract based on the provided summary, suitable for submission to a computer science conference in 2024:

**Abstract**

Non-Gaussian Component Analysis (NGCA) presents a significant challenge in dimensionality reduction and feature extraction, particularly when dealing with data exhibiting non-Gaussian distributions. This work introduces a rigorous theoretical framework for establishing near-optimal Statistical Query (SQ) lower bounds for NGCA, fundamentally challenging prior assumptions regarding the necessity of the chi-squared condition for achieving computational hardness. We demonstrate that the inherent complexity of the problem—specifically, the existence of a hidden subspace—guarantees a substantial resource requirement irrespective of the underlying data distribution’s Gaussianity. 

Our analysis leverages techniques from Non-Gaussian Component Analysis, employing a statistical query model to quantify the information-theoretic limits of estimation.  We derive lower bounds that approach optimal performance, providing a crucial benchmark for algorithm design and complexity assessment.  These results highlight a critical distinction: NGCA’s difficulty stems not from distributional assumptions, but from the structure of the hidden subspace, suggesting avenues for targeted approximation techniques and potentially novel algorithmic strategies.  Further research will explore the implications of these bounds across related statistical learning problems.