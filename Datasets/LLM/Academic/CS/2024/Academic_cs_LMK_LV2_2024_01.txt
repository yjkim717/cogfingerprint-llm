Title: Unveiling Polysemantic Behavior in CLIP Neurons through a Second-Order Lens

Abstract:
Contrastive Language-Image Pre-training (CLIP) models have demonstrated remarkable capabilities in multimodal representation learning. However, understanding the interpretability of CLIP neurons remains a challenging task. This study introduces a novel "second-order lens" to dissect the complex behavior of CLIP neurons, revealing their polysemantic nature. By examining the second-order effects within the CLIP architecture, we uncover the nuanced interactions between neurons that underlie their multifaceted representations. Our analysis exposes the polysemantic behavior of CLIP neurons, where a single neuron responds to multiple semantically distinct concepts. Leveraging this insight, we demonstrate the applicability of our approach in generating adversarial examples that exploit the polysemanticity of CLIP neurons. Furthermore, we showcase the potential for zero-shot segmentation tasks, highlighting the versatility of our second-order lens. This work advances the understanding of CLIP interpretability and paves the way for more robust and transparent multimodal models.