Input: Keywords: adversarial machine learning, model robustness, data poisoning, security vulnerabilities. Summary: This research investigates the escalating threat of adversarial machine learning attacks, specifically focusing on data poisoning techniques designed to induce security vulnerabilities within deployed models. We introduce a novel statistical test – the ‘Robustness Anomaly Score’ – to detect subtle shifts in training data indicative of malicious influence. Our analysis reveals that current defense mechanisms often exhibit limited efficacy against sophisticated poisoning strategies. This work proposes a framework for proactive regulatory oversight, utilizing anomaly detection to facilitate a timely regulatory audit of machine learning systems and mitigate emergent security risks within the evolving landscape of AI deployment in 2024.