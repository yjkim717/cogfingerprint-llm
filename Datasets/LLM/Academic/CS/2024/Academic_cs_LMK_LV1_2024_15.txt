Title: Mitigating Hallucinations in Open-Domain Question Answering with Adaptive Refusal Mechanisms

Abstract:
The deployment of AI assistants powered by large language models (LLMs) has significantly advanced open-domain question answering (QA) capabilities. However, these models are prone to generating factually incorrect responses, known as hallucinations, particularly when faced with unknown or unanswerable questions. To address this limitation, we propose an adaptive refusal mechanism that enables LLMs to abstain from answering questions when they are likely to produce hallucinations. By incorporating a calibrated confidence threshold, our approach allows AI assistants to refuse to answer uncertain questions, thereby reducing the incidence of factual errors. Our experiments demonstrate that this adaptive refusal mechanism substantially improves the truthfulness of LLM-based QA systems, achieving a significant reduction in hallucinations while maintaining a high level of accuracy on answerable questions. The results highlight the potential of this approach to enhance the reliability of AI assistants in real-world applications.