**Abstract**

The pervasive performance disparity of natural language processing (NLP) systems across dialectal variations remains a critical challenge for equitable language technology. While pre-trained language models (PLMs) have achieved state-of-the-art results on standardized language benchmarks, their generalization to non-dominant dialects is often poor, perpetuating linguistic bias. This paper presents a systematic analysis of dialect robustness in contemporary NLP. We first formalize a unified evaluation framework for dialect classification and subsequent understanding (NLU) and generation (NLG) tasks, spanning multiple languages and sociolinguistic contexts. Our empirical investigation reveals that current PLMs, despite their scale, encode and amplify dialectal hierarchies, leading to significant accuracy degradation and stereotyping in downstream applications. To address this, we propose and evaluate a suite of mitigation strategies, including dialect-adaptive fine-tuning and sociolinguistically-informed data augmentation. Our findings demonstrate that explicit modeling of dialectal features during pre-training and fine-tuning phases is essential for narrowing performance gaps. We conclude that advancing equitable NLP necessitates a fundamental shift from a monolingual paradigm to a multidialectal one, integrating dialectal sensitivity as a core objective in model development and evaluation.