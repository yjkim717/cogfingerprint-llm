**Abstract**

The deployment of large language models (LLMs) for long-context inference is critically constrained by the memory footprint of the Key-Value (KV) cache, which grows linearly with sequence length. While quantization offers a pathway to reduce this overhead, existing techniques often incur substantial degradation in model quality at sub-8-bit precision, particularly for the sensitive key cache. This paper introduces a novel quantization framework, **SpectraCache**, that addresses these limitations through a multi-faceted approach. We first propose **Spectral Key Quantization (SKQ)**, a non-uniform, per-channel method applied prior to rotary positional encoding (RoPE). This strategic placement capitalizes on the more Gaussian-like distribution of pre-RoPE keys, enabling highly accurate low-bit representation. To further compress the value cache, we develop **Adaptive Vector Quantization (AVQ)**, a per-vector scheme that dynamically segregates dense and sparse activation regions, applying tailored quantization parameters to each. Our co-designed techniques synergistically mitigate quantization error. Extensive evaluations on language modeling and long-context understanding benchmarks demonstrate that SpectraCache sustains near-fp16 performance at 3-bit precision, reducing KV cache memory by over 70%. This breakthrough facilitates inference for sequences exceeding 4 million tokens on contemporary hardware, substantially advancing the frontier of efficient long-context LLM serving in 2024.