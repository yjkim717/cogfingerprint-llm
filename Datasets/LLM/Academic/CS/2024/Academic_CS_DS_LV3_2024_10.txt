In 2024, the systematic comparison of supervised learning models remains a cornerstone of machine learning research. This paper addresses the critical challenge of selecting appropriate evaluation metrics and robust statistical tests to ensure statistically sound model comparisons, particularly within high-stakes domains such as medical imaging. We present a structured framework that aligns specific metrics—including calibrated uncertainty measures—with common supervised tasks, from classification to regression. Furthermore, we delineate a rigorous protocol for applying non-parametric statistical tests to validate performance differences, mitigating the risks of over-reliance on single metric scores. Our empirical analysis, conducted on several public benchmarks, demonstrates that this principled approach yields more reliable and reproducible conclusions than ad-hoc evaluation practices, thereby advancing methodological rigor in computational science.