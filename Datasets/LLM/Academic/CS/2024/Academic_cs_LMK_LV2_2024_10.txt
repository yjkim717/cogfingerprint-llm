Title: Evaluating Supervised Machine Learning Models: A Comparative Analysis of Metrics and Statistical Tests

Abstract:
The proliferation of machine learning (ML) models, particularly convolutional neural networks (CNNs), has necessitated a rigorous evaluation framework to compare their performance in supervised tasks. This study examines the efficacy of various evaluation metrics and statistical tests in assessing the performance of ML models. We conduct a comprehensive review of existing literature on evaluation metrics, including accuracy, precision, recall, F1-score, and AUC-ROC, and discuss their limitations. Furthermore, we investigate the application of statistical tests, such as paired t-tests and McNemar's test, to compare the performance of ML models. Our analysis reveals that the choice of evaluation metric and statistical test significantly impacts the comparison outcome. We provide guidelines for selecting suitable evaluation metrics and statistical tests for CNNs and other supervised ML tasks, informing best practices for model evaluation in 2024 and beyond.