**Abstract**

The proliferation of large language models (LLMs) as conversational AI assistants has been significantly hampered by their propensity to generate confident yet factually incorrect outputs, a phenomenon known as hallucination. This issue presents a critical challenge to the truthfulness and reliability of these systems, undermining their practical deployment. Current alignment techniques, primarily focused on general harm reduction, often fail to address the specific cognitive failure of overconfidence in the face of knowledge gaps. This paper proposes and evaluates a novel alignment methodology that directly targets this limitation. We demonstrate that fine-tuning LLMs on a carefully curated, model-specific "I don't know" dataset—comprising queries outside the model's verified knowledge boundaries—significantly enhances their self-awareness. This targeted intervention trains the model to recognize epistemic uncertainty, leading to a marked increase in appropriate refusals for unanswerable prompts. Our empirical results, conducted on multiple state-of-the-art architectures in 2024, confirm that this approach yields a statistically significant reduction in hallucination rates without compromising performance on answerable questions. We conclude that embedding calibrated epistemic humility through targeted data alignment is a vital and effective strategy for developing more truthful and reliable AI assistants.