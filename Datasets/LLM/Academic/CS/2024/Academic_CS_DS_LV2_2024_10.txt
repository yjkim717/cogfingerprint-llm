In the rapidly evolving landscape of supervised machine learning, robust model evaluation remains a cornerstone of empirical research. This paper systematically examines prevalent evaluation metrics—including accuracy, F1-score, and AUC-ROC—across classification and regression tasks, contextualizing their statistical properties and applicability domains. We further analyze rigorous statistical testing methodologies, such as paired t-tests and Wilcoxon signed-rank tests, to ascertain performance significance beyond metric disparities. Our 2024 perspective highlights emerging challenges in deep learning evaluation and addresses pitfalls in cross-validation protocols. By establishing a structured framework for metric selection and hypothesis testing, this work provides practitioners with principled guidelines for reproducible model comparison, ultimately advancing methodological rigor in machine learning research.