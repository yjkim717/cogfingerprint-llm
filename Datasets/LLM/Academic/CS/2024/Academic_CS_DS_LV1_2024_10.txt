**Abstract**

In the rapidly evolving domain of machine learning, the rigorous comparison of supervised models remains a critical challenge, particularly in high-stakes fields such as medical imaging. While numerous evaluation metrics exist, their interpretation often lacks statistical grounding, leading to potentially unreliable conclusions about model superiority. This paper addresses this gap by presenting a unified framework for model assessment that systematically integrates performance metrics with robust statistical testing. We delineate the appropriate application of metrics—including AUC-ROC, F1-score, and Brier score—across classification and regression tasks, contextualized with medical imaging case studies. Furthermore, we provide a critical analysis of statistical procedures, from non-parametric tests like the Wilcoxon signed-rank test for paired comparisons to correction methods for multiple hypothesis testing. Our work provides a principled methodology for researchers to derive statistically sound and clinically meaningful inferences from model evaluation, thereby enhancing the reliability of machine learning deployments in practice.