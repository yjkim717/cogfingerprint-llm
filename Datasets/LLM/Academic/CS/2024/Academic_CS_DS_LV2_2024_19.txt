This systematic review critically examines the state of generative artificial intelligence as of 2024, with particular emphasis on transformer-based large language models (LLMs) and generative adversarial networks (GANs) for image translation. Through rigorous analysis of peer-reviewed literature, we identify three dominant research trajectories: enhanced multimodal integration, where models process heterogeneous data types through unified architectures; improved controllability via reinforcement learning from human feedback (RLHF); and emergent reasoning capabilities in scaled LLMs. Our methodology employs comparative framework analysis across 127 studies published between 2022-2024, revealing that while parameter scaling continues to yield performance gains, architectural innovations in sparse attention mechanisms and diffusion models demonstrate greater computational efficiency. The review further establishes that current ethical challenges—including hallucination mitigation, copyright compliance, and bias propagation—require novel verification frameworks beyond conventional benchmarking. We conclude that the field's maturation necessitates developing standardized evaluation metrics that simultaneously assess technical proficiency, computational sustainability, and social impact, thereby aligning technical progress with responsible AI principles.