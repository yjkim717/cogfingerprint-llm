## Abstract: Mitigating Hallucination and Enhancing Reliability in Open-Domain Question Answering via Explicit Uncertainty Modeling

The increasing deployment of large language models (LLMs) as AI assistants necessitates a critical examination of their reliability, particularly concerning factual accuracy in open-domain question answering. A persistent challenge within this domain is the propensity of LLMs to generate plausible but verifiably false statements, commonly referred to as "hallucinations." This work investigates a novel approach to mitigate this issue by explicitly modeling knowledge uncertainty within LLMs. Our methodology centers on aligning LLMs with specialized datasets comprising instances where the correct response is "I don't know," effectively training the model to recognize and articulate its limitations. We hypothesize that such alignment fosters a more conservative and truthful response style, reducing the likelihood of generating fabricated information.  We present a detailed evaluation of this approach, demonstrating significant improvements in response accuracy and a demonstrable reduction in factual errors across a benchmark suite of open-domain question answering tasks.  Furthermore, we analyze the impact of different training strategies and dataset compositions on the modelâ€™s ability to appropriately signal uncertainty.  These findings contribute to the ongoing effort to develop more robust and trustworthy AI assistants, aligning with the growing demand for verifiable and reliable information retrieval in 2024 and beyond.