Of course. Here is an original abstract inspired by the provided concepts.

***

**Abstract**

The stochastic economic lot scheduling problem (SELSP) presents a persistent challenge in automated manufacturing, characterized by dynamic demand, sequence-dependent setups, and inherent production uncertainties. Traditional optimization and heuristic-based scheduling policies often exhibit limited adaptability in such non-stationary environments, leading to suboptimal throughput and elevated operational costs. This paper introduces a novel deep reinforcement learning (DRL) framework that leverages a transformer-based architecture to learn highly effective scheduling policies directly from operational data. Our approach formulates the SELSP as a partially observable Markov decision process, where the agent must sequentially decide which job to process next on a single, capacitated machine. The core of our method is an encoder-decoder model utilizing multi-head self-attention mechanisms, which enables the agent to dynamically weigh the complex interdependencies between heterogeneous job attributes—such as stochastic demand rates, due dates, and setup times—across the entire production horizon. This allows for the emergent learning of sophisticated, context-aware dispatching rules that far surpass the myopic decision-making of conventional methods. We evaluate our model against a suite of established scheduling heuristics and policy-based reinforcement learning baselines across a diverse set of simulated production scenarios. Empirical results demonstrate that our self-attention DRL agent consistently achieves significant improvements in key performance indicators, including a marked reduction in mean total cost and a substantial increase in service level adherence. The findings affirm the transformative potential of attention-based deep learning in developing resilient and scalable autonomous control systems for complex, stochastic industrial environments.