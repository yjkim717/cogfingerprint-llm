**Abstract: Mitigating Hallucinations in Open-Domain Question Answering via Explicit Uncertainty Modeling**

Recent advancements in large language models (LLMs) have yielded increasingly sophisticated AI assistants capable of engaging in open-domain question answering. However, a persistent challenge remains: the tendency of these models to generate confidently incorrect or fabricated responses – a phenomenon commonly referred to as “hallucination.” This study examines the efficacy of incorporating explicit uncertainty modeling to address this issue. We present an investigation into the ability of LLMs to recognize and articulate instances of lacking knowledge, utilizing an augmented dataset, termed the “Idk” dataset, designed to explicitly represent scenarios where a definitive answer is unavailable. 

Our methodology involved fine-tuning a state-of-the-art LLM and evaluating its performance on a diverse set of questions, with a particular focus on the frequency and quality of “I don’t know” responses. Preliminary results indicate a statistically significant improvement in the model’s propensity to express uncertainty when aligned with the Idk dataset, leading to enhanced overall accuracy and reduced hallucination rates. Future work will explore methods for dynamically adjusting the level of uncertainty expressed based on contextual factors.