This paper establishes fundamental computational barriers for structure learning in high-dimensional statistical models through the lens of statistical query (SQ) complexity. We investigate the problem of identifying latent low-dimensional structure within corrupted data distributions, focusing on settings where the signal component exhibits non-Gaussian characteristics. Our main contribution demonstrates that any SQ algorithm achieving consistent estimation in this framework requires super-polynomial sample complexity, even under significantly relaxed distributional assumptions compared to prior work. Specifically, we eliminate the need for stringent moment-matching conditions that previously appeared necessary for establishing such lower bounds. The results apply to a broad class of hidden subspace recovery problems and provide rigorous evidence for the computational-statistical gaps observed in high-dimensional inference. These findings suggest that efficient algorithms for robust non-Gaussian component analysis may require fundamentally new algorithmic approaches beyond current statistical query frameworks, particularly when dealing with adversarially corrupted or heavy-tailed data distributions.