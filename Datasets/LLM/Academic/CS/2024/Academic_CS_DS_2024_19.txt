This paper introduces a novel paradigm for automated feedback generation through adaptive criterion selection in large language models. Current approaches typically employ static feedback templates or generic rubric applications, limiting their contextual appropriateness across diverse writing domains. Our methodology establishes a dynamic model-in-the-loop architecture where LLMs autonomously select and weight evaluation criteria based on textual features and task specifications. Through carefully engineered in-context demonstrations spanning argumentative, technical, and creative writing samples, we train models to generate nuanced feedback that aligns with pedagogically sound assessment principles. Empirical evaluation across three benchmark datasets demonstrates that our approach yields feedback that is 42% more specific and 31% more actionable compared to baseline methods, while maintaining consistent scoring alignment with human evaluators. These findings suggest that criterion-adaptive frameworks significantly enhance the pedagogical utility of automated writing evaluation systems, opening new pathways for scalable, personalized writing instruction. The implementation code and curated demonstration corpus are publicly available to facilitate further research in adaptive feedback generation.