The pervasive deployment of natural language processing (NLP) systems has revealed significant performance disparities across dialectal variants of high-resource languages, posing a critical challenge for equitable language technology. This paper presents a systematic investigation into the architectural and data-centric factors within pre-trained language models that contribute to this performance degradation. We analyze how standard training paradigms, often optimized for monolithic, standardized language varieties, fail to capture the morphological, syntactic, and pragmatic nuances of regional and social dialects. Our work surveys contemporary methodologies for dialect-robust modeling, including adversarial training, dialect-aware data augmentation, and parameter-efficient fine-tuning strategies. Furthermore, we propose a novel evaluation framework for assessing model fairness across a dialect continuum, moving beyond discrete classification. The findings underscore that achieving genuine natural language understanding and generation parity necessitates a fundamental shift from dialect-agnostic to dialect-informed model design. This research contributes a critical pathway toward developing NLP systems that are not only more robust but also more equitable and inclusive for all speaker communities.