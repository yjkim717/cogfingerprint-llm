**Abstract**

The propensity of large language models (LLMs) to generate confident yet factually incorrect responses to unanswerable questions remains a critical challenge for their safe deployment. This research investigates the efficacy of model alignment using a novel "I Don't Know" (IDK) dataset to mitigate such hallucinations in open-domain question-answering tasks. We hypothesize that targeted fine-tuning on a curated corpus of queries lacking verifiable answers can instill a calibrated refusal mechanism. Our methodology involves constructing a specialized IDK dataset and applying instruction-tuning to a state-of-the-art LLM. Empirical results from a held-out test set demonstrate a statistically significant reduction in factual errors, quantified by a decrease in incorrect answer generation, without a commensurate loss in performance on answerable questions. This work substantiates that explicit training for uncertainty acknowledgment is a viable pathway for enhancing the truthfulness and reliability of AI assistants, directly addressing a key objective in AI safety alignment for 2024 and beyond.