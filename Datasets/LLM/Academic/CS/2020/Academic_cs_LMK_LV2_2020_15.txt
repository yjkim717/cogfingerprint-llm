This survey provides a comprehensive review of pre-trained language representation models in natural language processing (NLP). We categorize and analyze existing models, examining their adaptation to downstream NLP tasks. Our analysis highlights the strengths and limitations of current approaches, informing future research directions. As of 2020, our findings underscore the significance of representation learning in NLP, paving the way for advancements in the field.