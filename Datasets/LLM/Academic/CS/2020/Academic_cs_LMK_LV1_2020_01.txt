Title: Contrastive Learning for Unpaired Image-to-Image Translation via Mutual Information Maximization

Abstract:
Unpaired image-to-image translation has garnered significant attention in recent years, with existing methods often relying on adversarial losses and cycle-consistency constraints. However, these approaches can suffer from mode collapse and fail to capture nuanced semantic relationships between input and output images. In this work, we propose a novel contrastive learning framework that maximizes mutual information between input and output patches, leveraging a patch-based approach to capture fine-grained semantic correspondences. By formulating the translation task as a mutual information maximization problem, we encourage the generator to produce outputs that are highly correlated with the input, while avoiding the need for paired training data. Our method demonstrates state-of-the-art performance on several benchmark datasets, outperforming existing unpaired image-to-image translation methods in terms of both qualitative and quantitative metrics. Our results suggest that contrastive learning is a promising direction for improving the fidelity and diversity of unpaired image-to-image translation models.