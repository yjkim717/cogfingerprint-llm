This paper introduces a novel contrastive learning framework for unpaired image-to-image translation by maximizing mutual information between corresponding patches across domains. Unlike conventional approaches that rely on cycle-consistency constraints, our method establishes semantic correspondences through a patch-based contrastive objective, pulling positive pairs of patches from different domains closer while pushing negative patches apart in a learned feature space. We demonstrate that maximizing mutual information at the patch level enables more effective preservation of structural content during translation while facilitating domain-specific style transfer. Extensive experiments on multiple benchmark datasets validate that our approach achieves state-of-the-art performance in both quantitative metrics and qualitative assessments, significantly outperforming existing unpaired translation methods in preserving fine-grained details and structural coherence. This work establishes a new paradigm for unsupervised cross-domain translation through principled mutual information optimization.