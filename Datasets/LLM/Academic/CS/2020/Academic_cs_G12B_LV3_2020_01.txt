 Here's a formal academic-style abstract, inspired by the provided summary and keywords, suitable for a CS publication and reflecting a 2020 timeframe:

**Abstract**

Unpaired image-to-image translation presents a significant challenge in computer vision, often requiring extensive paired data or complex architectural designs. This paper proposes a novel contrastive learning framework to address this limitation, facilitating high-quality, one-sided translation even with single-image domain datasets. Our approach explicitly maximizes the mutual information between corresponding input and output image patches within a shared feature space, thereby enforcing semantic consistency during the translation process. By leveraging contrastive objectives, the model learns to align features effectively, reducing the reliance on paired data and accelerating training convergence. Experimental results demonstrate that our method achieves state-of-the-art performance on several benchmark datasets, exhibiting improved visual fidelity and structural coherence compared to existing unpaired translation techniques. This work contributes to a more efficient and versatile paradigm for image-to-image translation.