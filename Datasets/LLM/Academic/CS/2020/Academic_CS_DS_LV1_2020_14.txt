**Abstract**

This paper introduces a novel hierarchical reinforcement learning (HRL) framework for dynamic threat assessment in multi-vehicle autonomous driving scenarios. Conventional rule-based methods often fail to generalize in complex, unpredictable traffic interactions, creating a critical safety assurance gap. Our approach decomposes the threat assessment problem into two tiers: a high-level policy employs a Deep Q-Network (DQN) to perform strategic scene-level risk classification, while a low-level policy utilizes proximal policy optimization (PPO) to execute precise, real-time longitudinal and lateral control actions. The framework is trained and evaluated in a high-fidelity simulation environment that models stochastic driver behaviors, explicitly targeting the prevention of chain-reaction, multiple-vehicle collisions. Results from benchmark comparisons against a baseline Monte Carlo Tree Search (MCTS) method demonstrate a statistically significant 34% reduction in near-miss incidents and a marked improvement in trajectory smoothness, confirming the efficacy of the proposed HRL architecture for enhancing safety and robustness in dense traffic conditions.