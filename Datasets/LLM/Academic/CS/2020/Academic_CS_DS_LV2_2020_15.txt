This survey systematically examines the paradigm of pre-trained language models (PLMs) in natural language processing. We categorize contemporary architectures—autoregressive, autoencoding, and hybrid models—analyzing their mechanisms for learning contextualized representations. The paper investigates transfer learning methodologies for adapting these representations to downstream NLP tasks, highlighting techniques like fine-tuning and prompt-based learning. Our analysis identifies key research challenges, including computational efficiency, model interpretability, and multimodal extension. By synthesizing current advancements and limitations, this work provides a structured framework for understanding PLM capabilities while outlining critical directions for future investigation in representation learning and task-specific adaptation.