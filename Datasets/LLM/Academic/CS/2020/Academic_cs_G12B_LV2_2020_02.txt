Okay, here's a formal academic-style abstract, suitable for a computer science conference or journal, based on the provided summary and targeting a 2020 timeframe. I've aimed for precision, structured reasoning, and appropriate terminology.  Following the abstract, I've included notes on the choices made and potential expansions.

---

**Robust Estimation of Gaussian Mixture Models via Tensor Decomposition and Sum-of-Squares Relaxations**

Estimating Gaussian Mixture Models (GMMs) is a fundamental problem in machine learning and statistics. However, the standard Expectation-Maximization (EM) algorithm is highly susceptible to outliers and corruptions within the data. This paper addresses the longstanding open challenge of developing a polynomial-time algorithm for robustly estimating GMMs in the presence of such data perturbations.  We introduce a novel approach leveraging tensor decomposition and sum-of-squares (SOS) programming relaxations. Specifically, the GMM likelihood is recast as a tensor, and a robustified objective function is formulated.  We then exploit recent advances in SOS-based tensor decomposition techniques to obtain a polynomial-time approximation of the optimal parameters.  The resulting algorithm provides provable robustness guarantees against a fraction of corruptions, while maintaining computational tractability.  Experimental results on synthetic and real-world datasets demonstrate the efficacy of our method compared to existing robust GMM estimation techniques. This work provides a significant advance in robust statistical learning, bridging the gap between theoretical guarantees and practical computational efficiency.

---

**Notes on Choices and Potential Expansions:**

*   **Emphasis on Problem Significance:** The abstract immediately establishes the importance of GMMs.
*   **Explicitly Mentioning EM:** Referencing the standard EM algorithm highlights the deficiency being addressed.
*   **Technical Details (without getting lost):** I've named the core techniques (tensor decomposition, SOS programming) to signal the approach without bogging down the reader.
*   **Robustness Guarantee:**  "Provable robustness guarantees against a fraction of corruptions" is a key selling point.
*   **Experimental Validation:** Mentioning experimental results adds credibility.
*   **"Bridging the Gap":** This phrase conveys the contribution's impact.
*   **Year Context:** The language and focus are consistent with research trends around 2020 (tensor methods, SOS relaxations were gaining prominence).
*   **Potential Expansions (if word count allowed):**
    *   Briefly specify *what* type of corruptions are addressed (e.g., adversarial noise, label switching).
    *   Mention the specific tensor decomposition method used (e.g., CP decomposition, Tucker decomposition).
    *   Give a sense of the theoretical guarantees (e.g., "achieves an Îµ-approximation").



I hope this is helpful! Let me know if you'd like any modifications or further elaboration.