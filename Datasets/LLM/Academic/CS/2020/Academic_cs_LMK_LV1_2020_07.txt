Title: Rethinking Memorization in Machine Learning: Necessity and Implications for Prediction Problems

Abstract:
The efficacy of machine learning models is often attributed to their capacity to generalize from training data. However, the role of memorization in achieving high-accuracy learning remains a topic of debate. This paper investigates whether memorization of irrelevant training data is a necessary condition for high-performance prediction in certain natural problem domains. We formalize the notion of memorization through the lens of information complexity and analyze its relationship with prediction accuracy. By examining a range of benchmark datasets and employing a novel metric to quantify memorization, our results suggest that, for specific problem classes, memorization is not only unavoidable but also essential for achieving state-of-the-art performance. Our findings have significant implications for the design of machine learning algorithms and the interpretation of their results, particularly in contexts where training data may contain noise or irrelevant information.