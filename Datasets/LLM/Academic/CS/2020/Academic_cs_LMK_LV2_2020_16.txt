This paper presents a comprehensive survey of multilingual neural machine translation (MNMT), a rapidly evolving field that leverages transfer learning to improve machine translation performance across multiple languages. We categorize existing MNMT approaches into several paradigms, including data-driven and model-driven methods, and analyze their strengths and weaknesses. Our survey highlights the crucial role of multilingual parallel corpora in training MNMT models, enabling knowledge transfer and sharing across languages. We discuss the key challenges and opportunities in MNMT, including the need for more effective transfer learning methods, better handling of linguistic diversity, and improved evaluation metrics. By providing a structured overview of the current state-of-the-art in MNMT, this survey aims to inform future research directions and inspire innovations in this field. As of 2020, MNMT has shown significant promise in improving machine translation performance, and we identify potential avenues for further advancements.