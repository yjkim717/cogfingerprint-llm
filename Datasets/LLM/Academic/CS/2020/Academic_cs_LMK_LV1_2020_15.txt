Pre-trained models (PTMs) have revolutionized natural language processing (NLP) by providing contextualized representations that significantly enhance language understanding. This survey categorizes PTMs into distinct architectures, examining their language representation learning capabilities and adaptation mechanisms for downstream NLP tasks. We analyze the efficacy of PTMs across various tasks, highlighting their strengths and limitations. Furthermore, we outline promising future research directions, including the development of more efficient PTMs and their applications in multimodal and low-resource settings, thereby charting the trajectory of NLP research in the post-2020 era.