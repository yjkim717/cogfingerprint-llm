This 2020 survey systematically examines the paradigm shift in natural language processing driven by pre-trained models. We establish a comprehensive taxonomy analyzing architectural features, pre-training objectives, language representations, and application domains. The study delineates how these models learn transferable linguistic knowledge through self-supervised objectives on large corpora, subsequently enabling effective adaptation to diverse downstream tasks through fine-tuning and prompt-based strategies. Our analysis reveals emerging research trajectories including model compression for efficiency, multimodal integration, and enhanced interpretability. The survey concludes that pre-trained representations fundamentally reconfigure NLP methodology while highlighting critical challenges in computational sustainability and ethical deployment that warrant future investigation.