**Abstract**

We present a novel framework for unpaired image-to-image translation that leverages contrastive learning to enhance both the quality and efficiency of the mapping. Traditional adversarial methods often suffer from training instability and mode collapse. Our approach addresses this by reformulating the translation objective as a mutual information maximization problem. Specifically, we introduce a patch-based contrastive learning strategy where a query patch from the generated image is trained to be closer in an embedded feature space to its corresponding "positive" patch from the input image than to a multitude of other "negative" patches. This explicit constraint enforces semantic consistency at a local structural level, leading to more coherent and detailed output. Furthermore, we demonstrate that this formulation significantly stabilizes training and enables effective learning from a single image in each domain, substantially reducing data requirements. Our method achieves state-of-the-art results on several benchmark datasets, underscoring the efficacy of contrastive learning for representation disentanglement in image translation tasks.