**Abstract**

While contemporary Natural Language Processing (NLP) systems demonstrate remarkable proficiency in modeling lexical semantic similarity, their operational success often belies a fundamental limitation: a lack of perceptual and sensorimotor grounding. These models, predominantly trained on textual corpora, capture statistical patterns of word co-occurrence but fail to integrate the embodied, experiential knowledge that underpins human conceptual understanding. This paper posits that this grounding gap critically limits the robustness and generalizability of NLP systems, particularly when confronted with abstract concepts or contexts requiring real-world reasoning. We argue that the next frontier in computational linguistics lies in developing hybrid architectures that synergize distributional semantics from large-scale text with structured representations from perceptual and action-based modalities. By aligning with established psychological theories of embodied cognition, such an approach promises to yield AI systems with a more human-like, contextual, and functionally grounded comprehension of word meaning, moving beyond surface-level similarity towards genuine semantic understanding.