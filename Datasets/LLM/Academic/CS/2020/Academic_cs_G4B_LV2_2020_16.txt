**Abstract**

Multilingual Neural Machine Translation (MNMT) has emerged as a prominent paradigm within the field of Machine Translation (MT) since 2017, driven by advancements in deep learning and the availability of increasingly large parallel corpora. This 2020 survey synthesizes current research, offering a structured examination of MNMT approaches, broadly categorized by their underlying transfer learning methodologies. Specifically, we delineate between approaches leveraging shared parameter spaces via multilingual embeddings and those employing dedicated, task-specific modules.  A critical analysis reveals persistent challenges, including catastrophic forgetting, domain adaptation across languages, and the efficient utilization of limited low-resource language data. 

Furthermore, the survey investigates the impact of architectural innovations, such as adapter layers and cross-lingual pre-training, on MNMT performance.  Finally, we propose several key directions for future research, emphasizing the need for improved evaluation metrics that account for linguistic diversity and the development of robust techniques to mitigate the negative effects of language interference.  This work aims to provide a valuable resource for researchers and practitioners navigating the evolving landscape of MNMT.