This paper presents a novel contrastive learning framework for unpaired image-to-image translation, leveraging a patch-based approach to maximize mutual information between corresponding input and output patches. By optimizing the mutual information between associated patches, the proposed framework effectively captures contextual relationships between source and target domains. Experimental results demonstrate the efficacy of the proposed method in preserving input semantics and generating high-quality translations. The approach achieves state-of-the-art performance on various benchmarks, underscoring the potential of contrastive learning in unpaired image-to-image translation tasks, as evaluated in our 2020 study.