This paper presents a novel framework for learning stable deep dynamical systems through joint optimization of neural network dynamics and accompanying Lyapunov functions. While deep networks excel at modeling complex temporal dependencies, they typically lack formal stability guarantees critical for real-world applications. Our method constructs a learnable Lyapunov function alongside the dynamics model, enabling end-to-end training while ensuring global stability through constrained optimization. Theoretical analysis demonstrates that our approach maintains exponential stability guarantees throughout the learning process, even for high-dimensional systems. Empirical validation on video texture synthesis and robotic control tasks shows our models achieve competitive accuracy while providing formal stability certificates absent in conventional approaches. This work bridges the gap between expressive deep learning and rigorous dynamical systems theory, enabling reliable deployment in safety-critical domains where guaranteed stability is paramount.