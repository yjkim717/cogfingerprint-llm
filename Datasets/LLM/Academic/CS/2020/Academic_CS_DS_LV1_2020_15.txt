The proliferation of pre-trained language models has fundamentally reshaped the landscape of natural language processing. This survey systematically investigates the paradigm of language representation learning, analyzing model architectures, pre-training objectives, and knowledge transfer mechanisms. We critically examine strategies for adapting these representations to diverse downstream NLP tasks, highlighting the trade-offs between task-specific fine-tuning and prompt-based approaches. Our analysis identifies key research challenges, including computational efficiency, model robustness, and multimodal extension. By synthesizing current advancements and projecting future trajectories, this work provides a structured framework for understanding both the capabilities and limitations of pre-trained models, thereby informing subsequent innovation in the field.