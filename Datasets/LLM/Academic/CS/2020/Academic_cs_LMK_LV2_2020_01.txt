Title: Contrastive Learning for Unpaired Image-to-Image Translation via Mutual Information Maximization

Abstract:
We propose a novel contrastive learning framework for unpaired image-to-image translation, leveraging a patch-based approach to maximize mutual information between input and output images. By formulating the translation task as a mutual information maximization problem, our method effectively captures the underlying patterns and structures in the data. Specifically, we employ a contrastive loss function that encourages the model to identify corresponding patches between the input and output images, while distinguishing them from non-corresponding patches. This approach enables the model to learn a more accurate and robust representation of the translation task. Experimental results on various benchmark datasets demonstrate the effectiveness of our method, outperforming state-of-the-art unpaired image-to-image translation techniques. Our framework provides a new perspective on the role of contrastive learning in image-to-image translation, highlighting the importance of mutual information maximization in capturing the underlying data distribution.