Abstract

This comprehensive survey systematically investigates the paradigm of multilingual neural machine translation (MNMT), which leverages multilingual parallel corpora to enable knowledge transfer across translation directions. We critically analyze prevailing methodologies through the unifying lens of distributed representation learning, examining how end-to-end modeling architectures facilitate cross-lingual transfer. Our structured taxonomy distinguishes between (1) shared encoder-decoder frameworks that create interlingual representations, (2) language-specific component approaches with controlled parameter sharing, and (3) meta-learning strategies for rapid adaptation to new language pairs. The analysis reveals that while MNMT systems demonstrate remarkable zero-shot capabilities and resource efficiency for low-resource languages, they face persistent challenges including negative interference between linguistically distant languages and capacity bottlenecks in shared parameters. We further identify that current evaluation practices inadequately measure translation robustness and domain adaptability. Promising research directions emerging in 2020 include dynamic architecture learning, explicit linguistic constraint incorporation, and semi-supervised frameworks exploiting monolingual data. This synthesis provides foundational insights for developing more effective and scalable multilingual translation systems.