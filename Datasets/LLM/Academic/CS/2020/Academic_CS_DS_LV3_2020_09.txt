**Abstract**

The persistent challenge of source code comprehensibility extends beyond functional implementation to encompass the critical role of software comments. While comments are indispensable for documentation and maintenance, their textual quality—specifically readability—is frequently overlooked, potentially undermining their utility. This paper, situated in the context of 2020, addresses this gap by presenting a novel, automated tool designed to systematically evaluate and enhance the readability of inline comments. Our methodology is grounded in the adaptation of established linguistic metrics, most notably the Flesch Reading Ease and Flesch-Kincaid Grade Level indices, which we have recalibrated for the technical lexicon and syntactic conventions prevalent in software engineering. The tool integrates seamlessly into development environments, providing **programmers** with real-time, quantifiable feedback on comment clarity. A comparative evaluation was conducted to assess the tool's efficacy. The results demonstrate a statistically significant improvement in the reading ease of comments processed by our system compared to unmodified counterparts. For student cohorts, this translated to accelerated comprehension during code review tasks. For experienced **software** developers, the refactored comments reduced cognitive load and perceived complexity, facilitating more efficient debugging and knowledge transfer. This research underscores that comment quality is a measurable and optimizable asset. By leveraging computational **readability** metrics, we provide a scalable solution to a fundamental problem in **software** sustainability, empowering **programmers** to produce documentation that is not only technically accurate but also maximally intelligible.