>
Here's a formal academic abstract based on the provided summary, written in a CS style appropriate for a 2020 publication:

**Abstract**

Unpaired image-to-image translation aims to learn mappings between two distinct image domains without requiring paired training data, a significant challenge in computer vision. This paper introduces a novel contrastive learning framework designed to enhance the performance and efficiency of unpaired translation models. Our approach leverages a patch-based strategy to maximize the mutual information between corresponding image regions across domains. Specifically, we employ a contrastive loss function that encourages similar patches from aligned domains to be clustered together in a learned embedding space, while simultaneously pushing dissimilar patches apart. This mutual information maximization fosters domain invariance and promotes the generation of semantically consistent translations. Experimental results demonstrate that our framework achieves improved visual quality and reduced training time compared to existing unpaired image-to-image translation techniques, particularly in scenarios with complex domain shifts. The patch-based design further allows for efficient computation and scalability. We believe this work presents a promising direction for advancing unsupervised image translation by explicitly modeling domain relationships through contrastive learning.
