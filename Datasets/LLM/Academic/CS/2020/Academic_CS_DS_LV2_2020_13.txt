**Abstract**

Contemporary natural language processing (NLP) systems have achieved remarkable performance on tasks requiring lexical similarity judgments, demonstrating a capacity to model statistical relationships within text corpora. However, this success often masks a fundamental divergence from human semantic cognition. This article presents a comparative analysis of word meaning representations in humans and machines, contextualized by established psychological theories. We argue that while distributional semantic models excel at capturing similarity based on contextual co-occurrence, they exhibit critical limitations in *grounding*—the connection of symbols to perceptual and embodied experiences—and in conceptual flexibility. This lack of a grounded, experiential foundation restricts a model's ability to reason about novel scenarios or meanings beyond its training distribution. Consequently, we posit that the path toward more robust and human-like language understanding in AI necessitates architectures that integrate distributional information with mechanisms for perceptual and sensorimotor grounding, addressing a core gap identified in the state of the art circa 2020.