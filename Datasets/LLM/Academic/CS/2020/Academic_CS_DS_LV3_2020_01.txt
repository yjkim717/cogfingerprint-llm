This paper introduces a novel contrastive learning framework for unpaired image-to-image translation, addressing the fundamental challenge of preserving semantic content without paired training data. Our method leverages a patch-based approach to maximize mutual information between corresponding input and output image regions, establishing robust feature correspondences. This formulation enables effective one-sided translation, significantly reducing both model complexity and training time compared to cycle-consistent alternatives. Through extensive experiments on multiple benchmark datasets, we demonstrate that our approach achieves state-of-the-art performance in visual quality and semantic consistency. The proposed framework establishes a new paradigm for unsupervised representation learning in visual domains, with particular relevance for medical imaging and domain adaptation tasks where paired data remains scarce. Our code is publicly available to facilitate further research.