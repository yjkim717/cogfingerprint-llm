**Abstract**

The persistent limitations of unimodal biometric systems—notably their vulnerability to spoofing attacks and sensitivity to intra-class variations—have catalyzed research into multimodal fusion approaches. This paper, situated in the 2020 research landscape, presents a novel multimodal biometric authentication framework that synergistically integrates fingerprint and palmprint modalities via a deep neural network (DNN) architecture. The proposed system is engineered to mitigate the high False Acceptance Rate (FAR) and False Recognition Rate (FRR) endemic to single-modality systems by leveraging the distinct, complementary feature spaces of the two biometric traits. We employ a convolutional neural network (CNN) backbone for robust, hierarchical feature extraction from each modality, followed by a feature-level fusion strategy that consolidates the discriminative information into a unified, high-dimensional representation. This fused feature vector is subsequently processed by fully connected layers for final identity verification. Experimental results on benchmark datasets demonstrate a statistically significant enhancement in authentication accuracy and robustness compared to state-of-the-art unimodal and other fusion-based benchmarks. The system not only achieves a lower Equal Error Rate (EER) but also exhibits improved resilience against presentation attacks, thereby substantiating its efficacy for high-security applications. This research underscores the transformative potential of deep learning in advancing reliable and secure multimodal biometric solutions.