# test_pipeline.py
import os
from utils.file_utils import read_text, parse_metadata_from_path, build_llm_filename, write_text
from utils.extract_utils import extract_keywords_summary_count
from utils.prompt_utils import generate_prompt_from_summary
from utils.api_utils import chat
from config import get_llm_path

# âœ… 1. í…ŒìŠ¤íŠ¸ìš© Human íŒŒì¼ ê²½ë¡œ ì§€ì •
TEST_FILE = "Datasets/Human/Academic/Chem/2021/Academic_CHEMISTRY_2021_01.txt"

def run_test():
    if not os.path.exists(TEST_FILE):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {TEST_FILE}")
        return

    print(f"ğŸ“„ Testing file: {TEST_FILE}")
    text = read_text(TEST_FILE)
    meta = parse_metadata_from_path(TEST_FILE)

    # 2ï¸âƒ£ Keyword + Summary ì¶”ì¶œ
    print("\nğŸ§  Extracting keywords and summary...")
    extracted = extract_keywords_summary_count(text, meta["genre"], meta["subfield"], meta["year"])
    print(f"Keywords: {extracted['keywords']}")
    print(f"Summary: {extracted['summary']}")
    print(f"Word Count: {extracted['word_count']}")

    # 3ï¸âƒ£ Prompt ìƒì„±
    prompt = generate_prompt_from_summary(
        meta["genre"], meta["subfield"], meta["year"],
        extracted["keywords"], extracted["summary"], extracted["word_count"]
    )

    # 4ï¸âƒ£ DeepSeekìœ¼ë¡œ ìƒˆ ê¸€ ìƒì„±
    print("\nâœï¸ Generating new LLM text...")
    system_prompt = "You are a helpful assistant generating original academic text for research."
    llm_output = chat(system_prompt, prompt, max_tokens=1200)

    # 5ï¸âƒ£ ê²°ê³¼ ì €ì¥ (ì„ íƒ)
    out_dir = get_llm_path(meta["genre"], meta["subfield"], meta["year"])
    out_path = os.path.join(out_dir, build_llm_filename(meta))
    write_text(out_path, llm_output)
    print(f"\nâœ… Generated file saved to: {out_path}")

    # 6ï¸âƒ£ ì½˜ì†” ì¶œë ¥ (ë¯¸ë¦¬ë³´ê¸°)
    print("\n--- ğŸ”¹ LLM Output (Preview) ğŸ”¹ ---")
    print(llm_output[:1000])  # ì²˜ìŒ 1000ìë§Œ ë¯¸ë¦¬ë³´ê¸°

if __name__ == "__main__":
    run_test()
